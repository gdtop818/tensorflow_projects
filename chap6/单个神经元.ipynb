{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例22：交叉熵实验\n",
    "* softmax_cross_entropy_with_logits\n",
    "* -tf.reduce_sum(labels * tf.log(tf.nn.softxmax(logits)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-203be0c08ac4>:12: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "scaled= [[0.01791432 0.00399722 0.97808844]\n",
      " [0.04980332 0.04506391 0.90513283]]\n",
      "scaled2= [[0.21747023 0.21446465 0.56806517]\n",
      " [0.2300214  0.22893383 0.5410447 ]]\n",
      "rel1= [0.02215516 3.0996735 ] \n",
      "\n",
      "rel2= [0.56551915 1.4743223 ] \n",
      "\n",
      "rel3= [0.02215518 3.0996735 ]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "labels = [[0,0,1],[0,1,0]]\n",
    "logits = [[2,  0.5,6],\n",
    "          [0.1,0,  3]]\n",
    "logits_scaled = tf.nn.softmax(logits)\n",
    "logits_scaled2 = tf.nn.softmax(logits_scaled)\n",
    "\n",
    "\n",
    "result1 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "result2 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits_scaled)\n",
    "result3 = -tf.reduce_sum(labels*tf.log(logits_scaled),1)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (\"scaled=\",sess.run(logits_scaled))    \n",
    "    print (\"scaled2=\",sess.run(logits_scaled2)) #经过第二次的softmax后，分布概率会有变化\n",
    "    \n",
    "\n",
    "    print (\"rel1=\",sess.run(result1),\"\\n\")#正确的方式\n",
    "    print (\"rel2=\",sess.run(result2),\"\\n\")#如果将softmax变换完的值放进去会，就相当于算第二次softmax的loss，所以会出错\n",
    "    print (\"rel3=\",sess.run(result3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例23： one_hot实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel4= [2.1721554 2.7696736] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#标签总概率为1\n",
    "labels = [[0.4,0.1,0.5],[0.3,0.6,0.1]]\n",
    "result4 = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "with tf.Session() as sess:\n",
    "    print (\"rel4=\",sess.run(result4),\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例24：sparse交叉熵的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel5= [0.02215516 3.0996735 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sparse\n",
    "labels = [2,1] #其实是0 1 2 三个类。等价 第一行 001 第二行 010\n",
    "result5 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "with tf.Session() as sess:\n",
    "    print (\"rel5=\",sess.run(result5),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例25：计算loss值\n",
    "tf.reduce_mean(-tf.reduce_sum(labels * tf.log(logits_scaled),1) ) = tf.reduce_mean(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 1.5609143\n"
     ]
    }
   ],
   "source": [
    "#注意！！！这个函数的返回值并不是一个数，而是一个向量，\n",
    "#如果要求交叉熵loss，我们要对向量求均值，\n",
    "#就是对向量再做一步tf.reduce_mean操作    \n",
    "loss=tf.reduce_mean(result1)\n",
    "with tf.Session() as sess:\n",
    "    print (\"loss=\",sess.run(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss2= 1.5609144\n"
     ]
    }
   ],
   "source": [
    "labels = [[0,0,1],[0,1,0]]    \n",
    "loss2 = tf.reduce_mean(-tf.reduce_sum(labels * tf.log(logits_scaled),1) )\n",
    "with tf.Session() as sess:\n",
    "    print (\"loss2=\",sess.run(loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting H:/tensorflow_projects/chap6/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting H:/tensorflow_projects/chap6/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting H:/tensorflow_projects/chap6/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting H:/tensorflow_projects/chap6/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "输入数据: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "输入数据打shape: (55000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADydJREFUeJzt3X+QVfV5x/HPw7osCQQUjEgQgz8g\nFWGKdYNtsAmVmmoSg2mKkXYcOmNdk9GOmcl0tExnxMm0ITbROKkxWQMVZ4whk8RKiYk6yJQmWmQx\nRjBrI3FQFghoSAIYiyz79I89ZDa453sv9557z4Xn/Zpx9t7z3LPnmYufe+7d7/ner7m7AMQzouwG\nAJSD8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCOqkZh5spHX4KI1u5iGBUP5Pr+tNP2jVPLau\n8JvZZZLuktQm6evuviz1+FEarYtsfj2HBJCwwddW/dia3/abWZukuyVdLmmGpEVmNqPW3weguer5\nzD9H0lZ3f8nd35T0TUkLimkLQKPVE/7JkrYPud+Xbfs9ZtZlZj1m1nNIB+s4HIAi1RP+4f6o8Jb5\nwe7e7e6d7t7Zro46DgegSPWEv0/SlCH3z5C0s752ADRLPeHfKGmamZ1lZiMlXS1pdTFtAWi0mof6\n3L3fzG6U9KgGh/pWuPvzhXUGoKHqGud390ckPVJQLwCaiMt7gaAIPxAU4QeCIvxAUIQfCIrwA0ER\nfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB\nEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCKquVXrNbJuk/ZIOS+p3984imkLztM2Ynqy/8KlTkvUX\n//KeZH1AnlsbIUvu+5Vfn5Wsr7zjQ8n6hOVPJevR1RX+zJ+5+2sF/B4ATcTbfiCoesPvkh4zs01m\n1lVEQwCao963/XPdfaeZnSbpcTN7wd3XD31A9qLQJUmj9PY6DwegKHWd+d19Z/Zzj6SHJM0Z5jHd\n7t7p7p3t6qjncAAKVHP4zWy0mb3jyG1JH5S0pajGADRWPW/7J0p6yMyO/J5vuPsPCukKQMOZe/44\nbNHG2ni/yOY37XhRnDTljNzaT289Pbnvg5d8LVm/oGMgWR9R4c3jgPL3r2dfSVrz+oRkfcUlf5pb\n6+/bkdz3eLXB12qf701fQJFhqA8IivADQRF+ICjCDwRF+IGgCD8QVBGz+tBgL93+J8n6C39zd24t\nNaVWqjytdqDC+eF7vx2XrD994OxkPeXC0duS9Y+P2Zes73w0/5qzNeenpypHwJkfCIrwA0ERfiAo\nwg8ERfiBoAg/EBThB4JinP84sPDSHyXrqbH8StNiK73+3/3rc5L1x//i/GS9nqmzP7ri6mT9o19N\nf21418lbc2tr9N6aejqRcOYHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAY528Fc2Yly5+ckB7P/t5v\n87+eu9J8+i373pWsH/yHdybrP7+9LVmf/tn8JdoO976Y3HfUfz6drLd/LX3sQ4mvMthx8/uS+07+\n/JPJ+omAMz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFVxnN/MVkj6iKQ97j4z2zZe0ipJUyVtk3SV\nu/+qcW2e4J7enCx3ffxTyXrbrr25tcrz6X+RrO64OX2dQO8HvpysX37vdbm1tt7krvrlten1Cg75\npmQ99V0G737g5eS+/cnqiaGaM/99ki47atstkta6+zRJa7P7AI4jFcPv7uslHX1qWSBpZXZ7paQr\nC+4LQIPV+pl/orvvkqTs52nFtQSgGRp+bb+ZdUnqkqRRyr/OG0Bz1Xrm321mkyQp+7kn74Hu3u3u\nne7e2a6OGg8HoGi1hn+1pMXZ7cWSHi6mHQDNUjH8ZvagpKckvcfM+szsWknLJF1qZi9KujS7D+A4\nUvEzv7svyinNL7gX5PCN6esAGjkmPeq1xKR4Sd2/mZqsj9x9ILf20m3pOfX3XZO+hmCELFnfdDD/\n3FbPegInCq7wA4Ii/EBQhB8IivADQRF+ICjCDwTFV3efAN5YMCe3tvcP0v/ElYbyJmzOH6qTpK5x\n25L12Wvyp87O6Ugfu9Ly4hsTQ3mS9E/XJqYT65nkvhFw5geCIvxAUIQfCIrwA0ERfiAowg8ERfiB\noBjnPwHs/MSbubXeD6SX9640LXZA6bH4SvunxvLrmZIrSdd8+8Zk/ex1TyXr0XHmB4Ii/EBQhB8I\nivADQRF+ICjCDwRF+IGgGOc/wVWaE1/p9b+R+3dtvyS57/Z/nJasM45fH878QFCEHwiK8ANBEX4g\nKMIPBEX4gaAIPxBUxXF+M1sh6SOS9rj7zGzbUknXSXo1e9gSd3+kUU0i7V2rRubWFk6+IrnvzLE7\nk/VPTngyWZ/c9vZkPXV++fnnzkvu+bZ1T1f43ahHNWf++yRdNsz2O919dvYfwQeOMxXD7+7rJe1t\nQi8Amqiez/w3mtlzZrbCzE4prCMATVFr+O+RdI6k2ZJ2Sfpi3gPNrMvMesys55AO1ng4AEWrKfzu\nvtvdD7v7gKR7JeWuFOnu3e7e6e6d7eqotU8ABasp/GY2acjdj0naUkw7AJqlmqG+ByXNk3SqmfVJ\nulXSPDObLcklbZN0fQN7BNAA5p7+XvYijbXxfpHNb9rxUD9776xkff9nX0/Wn5i1Krd2254Lk/v+\n5IopyXp/345kPaINvlb7fG96QYQMV/gBQRF+ICjCDwRF+IGgCD8QFOEHguKru6t00pQzcmv92/ua\n2Elz+cbNyfqY4eZ7DrHwv/KnFD90bnoy6My/uzhZP3MpQ3314MwPBEX4gaAIPxAU4QeCIvxAUIQf\nCIrwA0Exzp95Y0HulxFJki5e+j+5tTUvn5/cd9KVvTX1dCL4zRfOzK0NfDU9nfzQtDeKbgdDcOYH\ngiL8QFCEHwiK8ANBEX4gKMIPBEX4gaDCjPOn5uNL0ic+9/1kvWff1Nxa5HH8tpPHJet/tezR3NoI\nVfUN02gQzvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFTFcX4zmyLpfkmnSxqQ1O3ud5nZeEmrJE2V\ntE3SVe7+q8a1Wp+X/zp/XrkkdY17OFm/88d/nls7Rz+uqafjwpz0Et2X//v6ZL3r5K25tYEK5572\nn70tWUd9qjnz90v6jLufJ+mPJd1gZjMk3SJprbtPk7Q2uw/gOFEx/O6+y92fyW7vl9QrabKkBZJW\nZg9bKenKRjUJoHjH9JnfzKZKukDSBkkT3X2XNPgCIem0opsD0DhVh9/Mxkj6jqRPu/u+Y9ivy8x6\nzKznkA7W0iOABqgq/GbWrsHgP+Du38027zazSVl9kqQ9w+3r7t3u3unune3qKKJnAAWoGH4zM0nL\nJfW6+x1DSqslLc5uL5aU/nM5gJZSzZTeuZKukbTZzJ7Nti2RtEzSt8zsWkmvSFrYmBaLMXnd/mS9\n/aa2ZP2m2U/k1pb//YeT+054Pv1x56QnNiXrlbTNmJ5b2zn/1OS+Yz78i2R93az7kvVK03JTw3nT\nv399ct/ptz2ZrKM+FcPv7j+Ucv+F5xfbDoBm4Qo/ICjCDwRF+IGgCD8QFOEHgiL8QFDmnl4muUhj\nbbxfZK05OnjgB2cn60/MWpVbG1HhNXRAA8n6bXsuTNYr+ei4/CnFF3Skj11v75X2f8+3b8itnfev\n25P79vftSNbxVht8rfb53qq+E50zPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTh/ptIS3n+4+pXc\n2r9MfC657yE/nKxXnhOf/jdK7V9p392H30jWv/LL9yXrj/3b3GR9wvKnknUUi3F+ABURfiAowg8E\nRfiBoAg/EBThB4Ii/EBQ1Xxvfwj92/uS9Z9cMSW3du7n65uP3zvv68n6+5+7Kll/de/Ymo997pf6\nk3XfuDlZnyDG8Y9XnPmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKiK8/nNbIqk+yWdLmlAUre732Vm\nSyVdJ+nV7KFL3P2R1O9q5fn8wIngWObzV3ORT7+kz7j7M2b2DkmbzOzxrHanu3+h1kYBlKdi+N19\nl6Rd2e39ZtYraXKjGwPQWMf0md/Mpkq6QNKGbNONZvacma0ws1Ny9ukysx4z6zmkg3U1C6A4VYff\nzMZI+o6kT7v7Pkn3SDpH0mwNvjP44nD7uXu3u3e6e2e7OgpoGUARqgq/mbVrMPgPuPt3Jcndd7v7\nYXcfkHSvpDmNaxNA0SqG38xM0nJJve5+x5Dtk4Y87GOSthTfHoBGqeav/XMlXSNps5k9m21bImmR\nmc2W5JK2Sbq+IR0CaIhq/tr/Q2nYL4ZPjukDaG1c4QcERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+\nICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq4ld3F3ows1clvTxk06mSXmtaA8emVXtr1b4keqtVkb29\n293fWc0Dmxr+txzcrMfdO0trIKFVe2vVviR6q1VZvfG2HwiK8ANBlR3+7pKPn9KqvbVqXxK91aqU\n3kr9zA+gPGWf+QGUpJTwm9llZva/ZrbVzG4po4c8ZrbNzDab2bNm1lNyLyvMbI+ZbRmybbyZPW5m\nL2Y/h10mraTelprZjuy5e9bMPlRSb1PMbJ2Z9ZrZ82Z2U7a91Ocu0Vcpz1vT3/abWZukn0m6VFKf\npI2SFrn7T5vaSA4z2yap091LHxM2s/dLOiDpfnefmW27XdJed1+WvXCe4u43t0hvSyUdKHvl5mxB\nmUlDV5aWdKWkv1WJz12ir6tUwvNWxpl/jqSt7v6Su78p6ZuSFpTQR8tz9/WS9h61eYGkldntlRr8\nn6fpcnprCe6+y92fyW7vl3RkZelSn7tEX6UoI/yTJW0fcr9PrbXkt0t6zMw2mVlX2c0MY2K2bPqR\n5dNPK7mfo1VcubmZjlpZumWeu1pWvC5aGeEfbvWfVhpymOvufyTpckk3ZG9vUZ2qVm5ulmFWlm4J\nta54XbQywt8nacqQ+2dI2llCH8Ny953Zzz2SHlLrrT68+8giqdnPPSX38zuttHLzcCtLqwWeu1Za\n8bqM8G+UNM3MzjKzkZKulrS6hD7ewsxGZ3+IkZmNlvRBtd7qw6slLc5uL5b0cIm9/J5WWbk5b2Vp\nlfzctdqK16Vc5JMNZXxJUpukFe7+z01vYhhmdrYGz/bS4CKm3yizNzN7UNI8Dc762i3pVkn/Ielb\nks6U9Iqkhe7e9D+85fQ2T4NvXX+3cvORz9hN7u1iSf8tabOkgWzzEg1+vi7tuUv0tUglPG9c4QcE\nxRV+QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeC+n9NtlByfRAtkgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cf48466c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据打shape: (10000, 784)\n",
      "输入数据打shape: (5000, 784)\n",
      "Epoch: 0001 cost= 8.143192529\n",
      "Epoch: 0002 cost= 4.322669148\n",
      "Epoch: 0003 cost= 2.981214518\n",
      "Epoch: 0004 cost= 2.356783852\n",
      "Epoch: 0005 cost= 1.998906395\n",
      "Epoch: 0006 cost= 1.765469893\n",
      "Epoch: 0007 cost= 1.600443805\n",
      "Epoch: 0008 cost= 1.477601487\n",
      "Epoch: 0009 cost= 1.381630285\n",
      "Epoch: 0010 cost= 1.305191407\n",
      "Epoch: 0011 cost= 1.241832566\n",
      "Epoch: 0012 cost= 1.188988984\n",
      "Epoch: 0013 cost= 1.143483993\n",
      "Epoch: 0014 cost= 1.104311068\n",
      "Epoch: 0015 cost= 1.069696186\n",
      "Epoch: 0016 cost= 1.039322816\n",
      "Epoch: 0017 cost= 1.012039655\n",
      "Epoch: 0018 cost= 0.987467080\n",
      "Epoch: 0019 cost= 0.965332884\n",
      "Epoch: 0020 cost= 0.945004881\n",
      "Epoch: 0021 cost= 0.926361536\n",
      "Epoch: 0022 cost= 0.909262278\n",
      "Epoch: 0023 cost= 0.893383189\n",
      "Epoch: 0024 cost= 0.878501318\n",
      "Epoch: 0025 cost= 0.864673607\n",
      " Finished!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"H:/tensorflow_projects/chap6/MNIST_data/\")\n",
    "\n",
    "print ('输入数据:',mnist.train.images)\n",
    "print ('输入数据打shape:',mnist.train.images.shape)\n",
    "\n",
    "import pylab \n",
    "im = mnist.train.images[1]\n",
    "im = im.reshape(-1,28)\n",
    "pylab.imshow(im)\n",
    "pylab.show()\n",
    "\n",
    "\n",
    "print ('输入数据打shape:',mnist.test.images.shape)\n",
    "print ('输入数据打shape:',mnist.validation.images.shape)\n",
    "\n",
    "\n",
    "import tensorflow as tf #导入tensorflow库\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data维度 28*28=784\n",
    "y = tf.placeholder(tf.int32, [None]) # 0-9 数字=> 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "z= tf.matmul(x, W) + b\n",
    "# 构建模型\n",
    "pred = tf.nn.softmax(z) # Softmax分类\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=z))\n",
    "#参数设置\n",
    "learning_rate = 0.01\n",
    "# 使用梯度下降优化器\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# 启动session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())# Initializing OP\n",
    "\n",
    "    # 启动循环开始训练\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # 遍历全部数据集\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # 显示训练中的详细信息\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print( \" Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例26 学习率衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "0.1\n",
      "1 0.1\n",
      "2 0.09791484\n",
      "3 0.09688862\n",
      "4 0.095873155\n",
      "5 0.094868325\n",
      "6 0.09387404\n",
      "7 0.092890166\n",
      "8 0.09191661\n",
      "9 0.09095325\n",
      "10 0.089999996\n",
      "11 0.08905673\n",
      "12 0.088123344\n",
      "13 0.08719975\n",
      "14 0.08628584\n",
      "15 0.0853815\n",
      "16 0.084486626\n",
      "17 0.08360115\n",
      "18 0.08272495\n",
      "19 0.08185792\n",
      "20 0.08099999\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "initial_learning_rate = 0.1 #初始学习率\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                           global_step,\n",
    "                                           decay_steps=10,decay_rate=0.9)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "add_global = global_step.assign_add(1)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(learning_rate))\n",
    "    for i in range(20):\n",
    "        g, rate = sess.run([add_global, learning_rate])\n",
    "        print(g,rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例27：·Maxout网络实现mnist分类.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting H:/tensorflow_projects/chap6/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting H:/tensorflow_projects/chap6/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting H:/tensorflow_projects/chap6/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting H:/tensorflow_projects/chap6/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "输入数据: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "输入数据打shape: (55000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cf48466208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据打shape: (10000, 784)\n",
      "输入数据打shape: (5000, 784)\n",
      "WARNING:tensorflow:From <ipython-input-10-69de2c9d2cb9>:33: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch: 0001 cost= 1.669748494\n",
      "Epoch: 0002 cost= 0.819802765\n",
      "Epoch: 0003 cost= 0.668256996\n",
      "Epoch: 0004 cost= 0.599882030\n",
      "Epoch: 0005 cost= 0.551539327\n",
      "Epoch: 0006 cost= 0.519114701\n",
      "Epoch: 0007 cost= 0.501650673\n",
      "Epoch: 0008 cost= 0.480439953\n",
      "Epoch: 0009 cost= 0.465431287\n",
      "Epoch: 0010 cost= 0.454214447\n",
      "Epoch: 0011 cost= 0.442614048\n",
      "Epoch: 0012 cost= 0.429748516\n",
      "Epoch: 0013 cost= 0.419512733\n",
      "Epoch: 0014 cost= 0.412809217\n",
      "Epoch: 0015 cost= 0.403128482\n",
      "Epoch: 0016 cost= 0.395945490\n",
      "Epoch: 0017 cost= 0.387481769\n",
      "Epoch: 0018 cost= 0.382592868\n",
      "Epoch: 0019 cost= 0.376352434\n",
      "Epoch: 0020 cost= 0.371442565\n",
      "Epoch: 0021 cost= 0.366640467\n",
      "Epoch: 0022 cost= 0.360618622\n",
      "Epoch: 0023 cost= 0.357322852\n",
      "Epoch: 0024 cost= 0.353282172\n",
      "Epoch: 0025 cost= 0.348204653\n",
      "Epoch: 0026 cost= 0.344141857\n",
      "Epoch: 0027 cost= 0.340688343\n",
      "Epoch: 0028 cost= 0.336875352\n",
      "Epoch: 0029 cost= 0.332229141\n",
      "Epoch: 0030 cost= 0.329368933\n",
      "Epoch: 0031 cost= 0.324990445\n",
      "Epoch: 0032 cost= 0.323535117\n",
      "Epoch: 0033 cost= 0.319696042\n",
      "Epoch: 0034 cost= 0.316543529\n",
      "Epoch: 0035 cost= 0.314367712\n",
      "Epoch: 0036 cost= 0.309627955\n",
      "Epoch: 0037 cost= 0.308954497\n",
      "Epoch: 0038 cost= 0.305743327\n",
      "Epoch: 0039 cost= 0.303948994\n",
      "Epoch: 0040 cost= 0.300707549\n",
      "Epoch: 0041 cost= 0.298111228\n",
      "Epoch: 0042 cost= 0.295571287\n",
      "Epoch: 0043 cost= 0.293599232\n",
      "Epoch: 0044 cost= 0.292371846\n",
      "Epoch: 0045 cost= 0.290433042\n",
      "Epoch: 0046 cost= 0.286466155\n",
      "Epoch: 0047 cost= 0.284913121\n",
      "Epoch: 0048 cost= 0.282463599\n",
      "Epoch: 0049 cost= 0.282443535\n",
      "Epoch: 0050 cost= 0.278840295\n",
      "Epoch: 0051 cost= 0.277910688\n",
      "Epoch: 0052 cost= 0.275044623\n",
      "Epoch: 0053 cost= 0.274304534\n",
      "Epoch: 0054 cost= 0.271387891\n",
      "Epoch: 0055 cost= 0.270530891\n",
      "Epoch: 0056 cost= 0.269293524\n",
      "Epoch: 0057 cost= 0.267875358\n",
      "Epoch: 0058 cost= 0.265286128\n",
      "Epoch: 0059 cost= 0.263074537\n",
      "Epoch: 0060 cost= 0.261540208\n",
      "Epoch: 0061 cost= 0.261259574\n",
      "Epoch: 0062 cost= 0.259737343\n",
      "Epoch: 0063 cost= 0.258162930\n",
      "Epoch: 0064 cost= 0.256089119\n",
      "Epoch: 0065 cost= 0.254655639\n",
      "Epoch: 0066 cost= 0.253505012\n",
      "Epoch: 0067 cost= 0.252484518\n",
      "Epoch: 0068 cost= 0.249667299\n",
      "Epoch: 0069 cost= 0.249462925\n",
      "Epoch: 0070 cost= 0.249046204\n",
      "Epoch: 0071 cost= 0.247562397\n",
      "Epoch: 0072 cost= 0.245829041\n",
      "Epoch: 0073 cost= 0.244501937\n",
      "Epoch: 0074 cost= 0.243986385\n",
      "Epoch: 0075 cost= 0.242621479\n",
      "Epoch: 0076 cost= 0.241314949\n",
      "Epoch: 0077 cost= 0.238647706\n",
      "Epoch: 0078 cost= 0.238957213\n",
      "Epoch: 0079 cost= 0.237347329\n",
      "Epoch: 0080 cost= 0.234964659\n",
      "Epoch: 0081 cost= 0.236123101\n",
      "Epoch: 0082 cost= 0.233973439\n",
      "Epoch: 0083 cost= 0.232953551\n",
      "Epoch: 0084 cost= 0.232046905\n",
      "Epoch: 0085 cost= 0.229982579\n",
      "Epoch: 0086 cost= 0.229070544\n",
      "Epoch: 0087 cost= 0.228393014\n",
      "Epoch: 0088 cost= 0.227479590\n",
      "Epoch: 0089 cost= 0.227268234\n",
      "Epoch: 0090 cost= 0.225049027\n",
      "Epoch: 0091 cost= 0.224516309\n",
      "Epoch: 0092 cost= 0.223888728\n",
      "Epoch: 0093 cost= 0.223191615\n",
      "Epoch: 0094 cost= 0.221796969\n",
      "Epoch: 0095 cost= 0.221250222\n",
      "Epoch: 0096 cost= 0.220323073\n",
      "Epoch: 0097 cost= 0.218742449\n",
      "Epoch: 0098 cost= 0.218513060\n",
      "Epoch: 0099 cost= 0.217564493\n",
      "Epoch: 0100 cost= 0.215474659\n",
      "Epoch: 0101 cost= 0.214555269\n",
      "Epoch: 0102 cost= 0.213661779\n",
      "Epoch: 0103 cost= 0.214191178\n",
      "Epoch: 0104 cost= 0.213189474\n",
      "Epoch: 0105 cost= 0.212041208\n",
      "Epoch: 0106 cost= 0.211847621\n",
      "Epoch: 0107 cost= 0.210278228\n",
      "Epoch: 0108 cost= 0.208721001\n",
      "Epoch: 0109 cost= 0.209450811\n",
      "Epoch: 0110 cost= 0.207888889\n",
      "Epoch: 0111 cost= 0.206186019\n",
      "Epoch: 0112 cost= 0.205807320\n",
      "Epoch: 0113 cost= 0.205915253\n",
      "Epoch: 0114 cost= 0.204875258\n",
      "Epoch: 0115 cost= 0.204274523\n",
      "Epoch: 0116 cost= 0.204331738\n",
      "Epoch: 0117 cost= 0.201808658\n",
      "Epoch: 0118 cost= 0.201525647\n",
      "Epoch: 0119 cost= 0.199703673\n",
      "Epoch: 0120 cost= 0.200700889\n",
      "Epoch: 0121 cost= 0.199350320\n",
      "Epoch: 0122 cost= 0.198106946\n",
      "Epoch: 0123 cost= 0.198094789\n",
      "Epoch: 0124 cost= 0.196696438\n",
      "Epoch: 0125 cost= 0.196361274\n",
      "Epoch: 0126 cost= 0.196492676\n",
      "Epoch: 0127 cost= 0.194797525\n",
      "Epoch: 0128 cost= 0.194349858\n",
      "Epoch: 0129 cost= 0.193110045\n",
      "Epoch: 0130 cost= 0.192708968\n",
      "Epoch: 0131 cost= 0.192399970\n",
      "Epoch: 0132 cost= 0.190516700\n",
      "Epoch: 0133 cost= 0.190331284\n",
      "Epoch: 0134 cost= 0.190980941\n",
      "Epoch: 0135 cost= 0.189532741\n",
      "Epoch: 0136 cost= 0.188812766\n",
      "Epoch: 0137 cost= 0.187239818\n",
      "Epoch: 0138 cost= 0.187442517\n",
      "Epoch: 0139 cost= 0.186436391\n",
      "Epoch: 0140 cost= 0.185879297\n",
      "Epoch: 0141 cost= 0.184914501\n",
      "Epoch: 0142 cost= 0.185321765\n",
      "Epoch: 0143 cost= 0.183773249\n",
      "Epoch: 0144 cost= 0.183931502\n",
      "Epoch: 0145 cost= 0.183287879\n",
      "Epoch: 0146 cost= 0.182621817\n",
      "Epoch: 0147 cost= 0.181577222\n",
      "Epoch: 0148 cost= 0.180124871\n",
      "Epoch: 0149 cost= 0.181275859\n",
      "Epoch: 0150 cost= 0.180238542\n",
      "Epoch: 0151 cost= 0.178712672\n",
      "Epoch: 0152 cost= 0.178188846\n",
      "Epoch: 0153 cost= 0.177580589\n",
      "Epoch: 0154 cost= 0.177027715\n",
      "Epoch: 0155 cost= 0.177836312\n",
      "Epoch: 0156 cost= 0.176792373\n",
      "Epoch: 0157 cost= 0.175756311\n",
      "Epoch: 0158 cost= 0.174947099\n",
      "Epoch: 0159 cost= 0.174266882\n",
      "Epoch: 0160 cost= 0.174342527\n",
      "Epoch: 0161 cost= 0.172602550\n",
      "Epoch: 0162 cost= 0.172811079\n",
      "Epoch: 0163 cost= 0.172335094\n",
      "Epoch: 0164 cost= 0.171968882\n",
      "Epoch: 0165 cost= 0.171027398\n",
      "Epoch: 0166 cost= 0.169943000\n",
      "Epoch: 0167 cost= 0.170124644\n",
      "Epoch: 0168 cost= 0.168496490\n",
      "Epoch: 0169 cost= 0.169623626\n",
      "Epoch: 0170 cost= 0.168593532\n",
      "Epoch: 0171 cost= 0.167650817\n",
      "Epoch: 0172 cost= 0.167899388\n",
      "Epoch: 0173 cost= 0.166965650\n",
      "Epoch: 0174 cost= 0.166645279\n",
      "Epoch: 0175 cost= 0.166120962\n",
      "Epoch: 0176 cost= 0.165155771\n",
      "Epoch: 0177 cost= 0.165017686\n",
      "Epoch: 0178 cost= 0.163808241\n",
      "Epoch: 0179 cost= 0.163797412\n",
      "Epoch: 0180 cost= 0.162719157\n",
      "Epoch: 0181 cost= 0.163193959\n",
      "Epoch: 0182 cost= 0.161633140\n",
      "Epoch: 0183 cost= 0.162454181\n",
      "Epoch: 0184 cost= 0.161832177\n",
      "Epoch: 0185 cost= 0.161416251\n",
      "Epoch: 0186 cost= 0.159936835\n",
      "Epoch: 0187 cost= 0.160258861\n",
      "Epoch: 0188 cost= 0.159245104\n",
      "Epoch: 0189 cost= 0.158908117\n",
      "Epoch: 0190 cost= 0.157777246\n",
      "Epoch: 0191 cost= 0.157958048\n",
      "Epoch: 0192 cost= 0.157402902\n",
      "Epoch: 0193 cost= 0.157361584\n",
      "Epoch: 0194 cost= 0.156321988\n",
      "Epoch: 0195 cost= 0.156084833\n",
      "Epoch: 0196 cost= 0.155017134\n",
      "Epoch: 0197 cost= 0.155896032\n",
      "Epoch: 0198 cost= 0.154472644\n",
      "Epoch: 0199 cost= 0.154645715\n",
      "Epoch: 0200 cost= 0.153077820\n",
      " Finished!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"H:/tensorflow_projects/chap6/MNIST_data/\")\n",
    "\n",
    "print ('输入数据:',mnist.train.images)\n",
    "print ('输入数据打shape:',mnist.train.images.shape)\n",
    "\n",
    "import pylab \n",
    "im = mnist.train.images[1]\n",
    "im = im.reshape(-1,28)\n",
    "pylab.imshow(im)\n",
    "pylab.show()\n",
    "\n",
    "\n",
    "print ('输入数据打shape:',mnist.test.images.shape)\n",
    "print ('输入数据打shape:',mnist.validation.images.shape)\n",
    "\n",
    "\n",
    "import tensorflow as tf #导入tensorflow库\n",
    "\n",
    "def max_out(inputs, num_units, axis=None):\n",
    "    shape = inputs.get_shape().as_list()\n",
    "    if shape[0] is None:\n",
    "        shape[0] = -1\n",
    "    if axis is None:  # Assume that channel is the last dimension\n",
    "        axis = -1\n",
    "    num_channels = shape[axis]\n",
    "    if num_channels % num_units:\n",
    "        raise ValueError('number of features({}) is not '\n",
    "                         'a multiple of num_units({})'.format(num_channels, num_units))\n",
    "    shape[axis] = num_units\n",
    "    shape += [num_channels // num_units]\n",
    "    outputs = tf.reduce_max(tf.reshape(inputs, shape), -1, keep_dims=False)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data维度 28*28=784\n",
    "y = tf.placeholder(tf.int32, [None]) # 0-9 数字=> 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.random_normal([784, 100]))\n",
    "b = tf.Variable(tf.zeros([100]))\n",
    "\n",
    "\n",
    "z= tf.matmul(x, W) + b\n",
    "#maxout = tf.reduce_max(z,axis= 1,keep_dims=True)\n",
    "\n",
    "maxout= max_out(z, 50)\n",
    "\n",
    "# Set model weights\n",
    "W2 = tf.Variable(tf.truncated_normal([50, 10], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "# 构建模型\n",
    "#pred = tf.nn.softmax(tf.matmul(maxout, W2) + b2)\n",
    "pred = tf.matmul(maxout, W2) + b2\n",
    "# 构建模型\n",
    "#pred = tf.nn.softmax(z) # Softmax分类\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
    "\n",
    "#参数设置\n",
    "learning_rate = 0.04\n",
    "# 使用梯度下降优化器\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "training_epochs = 200\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# 启动session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())# Initializing OP\n",
    "\n",
    "    # 启动循环开始训练\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # 遍历全部数据集\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # 显示训练中的详细信息\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print( \" Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
