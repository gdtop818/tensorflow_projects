{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例37：卷积函数的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "op1:\n",
      " [array([[[[-2.],\n",
      "         [-2.],\n",
      "         [-1.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [-2.],\n",
      "         [-1.]],\n",
      "\n",
      "        [[-1.],\n",
      "         [-1.],\n",
      "         [-1.]]]], dtype=float32), array([[[[-1.]],\n",
      "\n",
      "        [[ 0.]]],\n",
      "\n",
      "\n",
      "       [[[ 0.]],\n",
      "\n",
      "        [[-1.]]]], dtype=float32)]\n",
      "------------------\n",
      "op2:\n",
      " [array([[[[-2., -2.],\n",
      "         [-2., -2.],\n",
      "         [-2.,  0.]],\n",
      "\n",
      "        [[-2., -2.],\n",
      "         [-2., -2.],\n",
      "         [-2.,  0.]],\n",
      "\n",
      "        [[-1., -1.],\n",
      "         [-1., -1.],\n",
      "         [-1.,  0.]]]], dtype=float32), array([[[[-1.,  0.]],\n",
      "\n",
      "        [[ 0., -1.]]],\n",
      "\n",
      "\n",
      "       [[[-1.,  0.]],\n",
      "\n",
      "        [[ 0., -1.]]]], dtype=float32)]\n",
      "op3:\n",
      " [array([[[[-2., -2., -2.],\n",
      "         [-2., -2., -2.],\n",
      "         [-1., -1., -1.]],\n",
      "\n",
      "        [[-2., -2., -2.],\n",
      "         [-2., -2., -2.],\n",
      "         [-1., -1., -1.]],\n",
      "\n",
      "        [[-2., -1.,  0.],\n",
      "         [-2., -1.,  0.],\n",
      "         [-1.,  0.,  0.]]]], dtype=float32), array([[[[-1.,  0.,  0.]],\n",
      "\n",
      "        [[-1., -1.,  0.]]],\n",
      "\n",
      "\n",
      "       [[[ 0., -1., -1.]],\n",
      "\n",
      "        [[ 0.,  0., -1.]]]], dtype=float32)]\n",
      "------------------\n",
      "op4:\n",
      " [array([[[[-4., -4.],\n",
      "         [-4., -4.],\n",
      "         [-2., -2.]],\n",
      "\n",
      "        [[-4., -4.],\n",
      "         [-4., -4.],\n",
      "         [-2., -2.]],\n",
      "\n",
      "        [[-2., -2.],\n",
      "         [-2., -2.],\n",
      "         [-1., -1.]]]], dtype=float32), array([[[[-1.,  0.],\n",
      "         [ 0., -1.]],\n",
      "\n",
      "        [[-1.,  0.],\n",
      "         [ 0., -1.]]],\n",
      "\n",
      "\n",
      "       [[[-1.,  0.],\n",
      "         [ 0., -1.]],\n",
      "\n",
      "        [[-1.,  0.],\n",
      "         [ 0., -1.]]]], dtype=float32)]\n",
      "op5:\n",
      " [array([[[[-4.],\n",
      "         [-4.],\n",
      "         [-2.]],\n",
      "\n",
      "        [[-4.],\n",
      "         [-4.],\n",
      "         [-2.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [-2.],\n",
      "         [-1.]]]], dtype=float32), array([[[[-1.],\n",
      "         [ 0.]],\n",
      "\n",
      "        [[ 0.],\n",
      "         [-1.]]],\n",
      "\n",
      "\n",
      "       [[[-1.],\n",
      "         [ 0.]],\n",
      "\n",
      "        [[ 0.],\n",
      "         [-1.]]]], dtype=float32)]\n",
      "------------------\n",
      "op1:\n",
      " [array([[[[-2.],\n",
      "         [-2.],\n",
      "         [-1.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [-2.],\n",
      "         [-1.]],\n",
      "\n",
      "        [[-1.],\n",
      "         [-1.],\n",
      "         [-1.]]]], dtype=float32), array([[[[-1.]],\n",
      "\n",
      "        [[ 0.]]],\n",
      "\n",
      "\n",
      "       [[[ 0.]],\n",
      "\n",
      "        [[-1.]]]], dtype=float32)]\n",
      "vop1:\n",
      " [array([[[[-2.],\n",
      "         [-2.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [-2.]]]], dtype=float32), array([[[[-1.]],\n",
      "\n",
      "        [[ 0.]]],\n",
      "\n",
      "\n",
      "       [[[ 0.]],\n",
      "\n",
      "        [[-1.]]]], dtype=float32)]\n",
      "op6:\n",
      " [array([[[[-2.],\n",
      "         [-2.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [-2.]]]], dtype=float32), array([[[[-1.]],\n",
      "\n",
      "        [[ 0.]]],\n",
      "\n",
      "\n",
      "       [[[ 0.]],\n",
      "\n",
      "        [[-1.]]]], dtype=float32)]\n",
      "vop6:\n",
      " [array([[[[-2.],\n",
      "         [-2.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [-2.]]]], dtype=float32), array([[[[-1.]],\n",
      "\n",
      "        [[ 0.]]],\n",
      "\n",
      "\n",
      "       [[[ 0.]],\n",
      "\n",
      "        [[-1.]]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf  \n",
    "\n",
    "# 1. 定义输入变量\n",
    "# [batch, in_height, in_width, in_channels] [训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]  \n",
    "input = tf.Variable(tf.constant(1.0,shape = [1, 5, 5, 1])) \n",
    "input2 = tf.Variable(tf.constant(1.0,shape = [1, 5, 5, 2]))\n",
    "input3 = tf.Variable(tf.constant(1.0,shape = [1, 4, 4, 1])) \n",
    "\n",
    "# 2. 定义卷积核变量\n",
    "# [filter_height, filter_width, in_channels, out_channels] [卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]   \n",
    "filter1 =  tf.Variable(tf.constant([-1.0,0,0,-1],shape = [2, 2, 1, 1]))\n",
    "filter2 =  tf.Variable(tf.constant([-1.0,0,0,-1,-1.0,0,0,-1],shape = [2, 2, 1, 2])) \n",
    "filter3 =  tf.Variable(tf.constant([-1.0,0,0,-1,-1.0,0,0,-1,-1.0,0,0,-1],shape = [2, 2, 1, 3])) \n",
    "filter4 =  tf.Variable(tf.constant([-1.0,0,0,-1,\n",
    "                                   -1.0,0,0,-1,\n",
    "                                   -1.0,0,0,-1,\n",
    "                                   -1.0,0,0,-1],shape = [2, 2, 2, 2])) \n",
    "filter5 =  tf.Variable(tf.constant([-1.0,0,0,-1,-1.0,0,0,-1],shape = [2, 2, 2, 1])) \n",
    "\n",
    "\n",
    "# 3. 定义卷积操作\n",
    "# padding的值为‘VALID’，表示边缘不填充, 当其为‘SAME’时，表示填充到卷积核可以到达图像边缘  \n",
    "op1 = tf.nn.conv2d(input, filter1, strides=[1, 2, 2, 1], padding='SAME') #1个通道输入，生成1个feature ma\n",
    "op2 = tf.nn.conv2d(input, filter2, strides=[1, 2, 2, 1], padding='SAME') #1个通道输入，生成2个feature map\n",
    "op3 = tf.nn.conv2d(input, filter3, strides=[1, 2, 2, 1], padding='SAME') #1个通道输入，生成3个feature map\n",
    "\n",
    "op4 = tf.nn.conv2d(input2, filter4, strides=[1, 2, 2, 1], padding='SAME') # 2个通道输入，生成2个feature\n",
    "op5 = tf.nn.conv2d(input2, filter5, strides=[1, 2, 2, 1], padding='SAME') # 2个通道输入，生成一个feature map\n",
    "\n",
    "vop1 = tf.nn.conv2d(input, filter1, strides=[1, 2, 2, 1], padding='VALID') # 5*5 对于pading不同而不同\n",
    "op6 = tf.nn.conv2d(input3, filter1, strides=[1, 2, 2, 1], padding='SAME') \n",
    "vop6 = tf.nn.conv2d(input3, filter1, strides=[1, 2, 2, 1], padding='VALID')  #4*4与pading无关\n",
    "  \n",
    "\n",
    "# 4. 运行卷积操作\n",
    "init = tf.global_variables_initializer()  \n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)  \n",
    "    \n",
    "    print(\"op1:\\n\",sess.run([op1,filter1]))#1-1  后面补0\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    print(\"op2:\\n\",sess.run([op2,filter2])) #1-2多卷积核 按列取\n",
    "    print(\"op3:\\n\",sess.run([op3,filter3])) #1-3\n",
    "    print(\"------------------\")   \n",
    "    \n",
    "    print(\"op4:\\n\",sess.run([op4,filter4]))#2-2    通道叠加\n",
    "    print(\"op5:\\n\",sess.run([op5,filter5]))#2-1        \n",
    "    print(\"------------------\")\n",
    "  \n",
    "    print(\"op1:\\n\",sess.run([op1,filter1]))#1-1\n",
    "    print(\"vop1:\\n\",sess.run([vop1,filter1]))\n",
    "    print(\"op6:\\n\",sess.run([op6,filter1]))\n",
    "    print(\"vop6:\\n\",sess.run([vop6,filter1]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例38：使用卷积提取图片的轮廓（sobel算子）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD8CAYAAACM5bN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aaa9638cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 121, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD8CAYAAACM5bN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngJoiCGdAQE0RhDMgoKb4fy11HfoKM6lYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ab4c7562e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 1. 载入图片并显示\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "import matplotlib.image as mpimg # mpimg 用于读取图片\n",
    "import numpy as np\n",
    "import tensorflow as tf  \n",
    "\n",
    "myimg = mpimg.imread('img.jpg') # 读取和代码处于同一目录下的图片\n",
    "plt.imshow(myimg) # 显示图片\n",
    "plt.axis('off') # 不显示坐标轴\n",
    "plt.show()\n",
    "print(myimg.shape)\n",
    "\n",
    "# 2. 定义占位符、卷积核、卷积op\n",
    "full=np.reshape(myimg,[1, 140, 121, 3])  \n",
    "inputfull = tf.Variable(tf.constant(1.0,shape = [1, 140, 121, 3]))\n",
    "\n",
    "filter =  tf.Variable(tf.constant([[-1.0,-1.0,-1.0],  [0,0,0],  [1.0,1.0,1.0],\n",
    "                                    [-2.0,-2.0,-2.0], [0,0,0],  [2.0,2.0,2.0],\n",
    "                                    [-1.0,-1.0,-1.0], [0,0,0],  [1.0,1.0,1.0]],shape = [3, 3, 3, 1]))                                    \n",
    "\n",
    "op = tf.nn.conv2d(inputfull, filter, strides=[1, 1, 1, 1], padding='SAME') #3个通道输入，生成1个feature ma\n",
    "o=tf.cast(  ((op-tf.reduce_min(op))/(tf.reduce_max(op)-tf.reduce_min(op)) ) *255 ,tf.uint8)\n",
    "\n",
    "\n",
    "# 3. 运行卷积操作并显示\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer()  )  \n",
    "\n",
    "    t,f=sess.run([o,filter],feed_dict={ inputfull:full})\n",
    "    #print(f)\n",
    "    t=np.reshape(t,[140, 121]) \n",
    " \n",
    "    plt.imshow(t,cmap='Greys_r') # 显示图片\n",
    "    plt.axis('off') # 不显示坐标轴\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例39：池化函数的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:\n",
      "[[[[0. 4.]\n",
      "   [0. 4.]\n",
      "   [0. 4.]\n",
      "   [0. 4.]]\n",
      "\n",
      "  [[1. 5.]\n",
      "   [1. 5.]\n",
      "   [1. 5.]\n",
      "   [1. 5.]]\n",
      "\n",
      "  [[2. 6.]\n",
      "   [2. 6.]\n",
      "   [2. 6.]\n",
      "   [2. 6.]]\n",
      "\n",
      "  [[3. 7.]\n",
      "   [3. 7.]\n",
      "   [3. 7.]\n",
      "   [3. 7.]]]]\n",
      "reslut:\n",
      " [[[[1. 5.]\n",
      "   [1. 5.]]\n",
      "\n",
      "  [[3. 7.]\n",
      "   [3. 7.]]]]\n",
      "reslut1:\n",
      " [[[[1. 5.]\n",
      "   [1. 5.]\n",
      "   [1. 5.]]\n",
      "\n",
      "  [[2. 6.]\n",
      "   [2. 6.]\n",
      "   [2. 6.]]\n",
      "\n",
      "  [[3. 7.]\n",
      "   [3. 7.]\n",
      "   [3. 7.]]]]\n",
      "reslut2:\n",
      " [[[[1.  5. ]\n",
      "   [1.  5. ]\n",
      "   [1.  5. ]\n",
      "   [1.  5. ]]\n",
      "\n",
      "  [[1.5 5.5]\n",
      "   [1.5 5.5]\n",
      "   [1.5 5.5]\n",
      "   [1.5 5.5]]\n",
      "\n",
      "  [[2.  6. ]\n",
      "   [2.  6. ]\n",
      "   [2.  6. ]\n",
      "   [2.  6. ]]\n",
      "\n",
      "  [[2.5 6.5]\n",
      "   [2.5 6.5]\n",
      "   [2.5 6.5]\n",
      "   [2.5 6.5]]]]\n",
      "reslut3:\n",
      " [[[[1.5 5.5]]]]\n",
      "reslut4:\n",
      " [1.5 5.5]\n",
      "flat:\n",
      " [[0. 1. 2. 3. 0. 1. 2. 3. 0. 1. 2. 3. 0. 1. 2. 3.]\n",
      " [4. 5. 6. 7. 4. 5. 6. 7. 4. 5. 6. 7. 4. 5. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf  \n",
    "\n",
    "# 1. 定义输入变量 (input, ksize, strides, padding, name = None)\n",
    "img=tf.constant([  \n",
    "        [[0.0,4.0],[0.0,4.0],[0.0,4.0],[0.0,4.0]],  \n",
    "        [[1.0,5.0],[1.0,5.0],[1.0,5.0],[1.0,5.0]],  \n",
    "        [[2.0,6.0],[2.0,6.0],[2.0,6.0],[2.0,6.0]],  \n",
    "        [[3.0,7.0],[3.0,7.0], [3.0,7.0],[3.0,7.0]]\n",
    "    ])  \n",
    "  \n",
    "img=tf.reshape(img,[1,4,4,2])  \n",
    "\n",
    "# 2. 定义池化操作\n",
    "pooling=tf.nn.max_pool(img,[1,2,2,1],[1,2,2,1],padding='VALID')  \n",
    "pooling1=tf.nn.max_pool(img,[1,2,2,1],[1,1,1,1],padding='VALID')\n",
    "pooling2=tf.nn.avg_pool(img,[1,4,4,1],[1,1,1,1],padding='SAME')  \n",
    "pooling3=tf.nn.avg_pool(img,[1,4,4,1],[1,4,4,1],padding='SAME') \n",
    "nt_hpool2_flat = tf.reshape(tf.transpose(img), [-1, 16]) \n",
    "pooling4=tf.reduce_mean(nt_hpool2_flat,1) #1对行求均值（1表示轴是列）   0 对列求均值\n",
    "\n",
    "# 3. 运行池化操作\n",
    "with tf.Session() as sess:  \n",
    "    print(\"image:\")  \n",
    "    image=sess.run(img)  \n",
    "    print (image)  \n",
    "    result=sess.run(pooling)  \n",
    "    print (\"reslut:\\n\",result)  \n",
    "    result=sess.run(pooling1)  \n",
    "    print (\"reslut1:\\n\",result)     \n",
    "    result=sess.run(pooling2)  \n",
    "    print (\"reslut2:\\n\",result)\n",
    "    result=sess.run(pooling3)  \n",
    "    print (\"reslut3:\\n\",result) \n",
    "    flat,result=sess.run([nt_hpool2_flat,pooling4])  \n",
    "    print (\"reslut4:\\n\",result) \n",
    "    print(\"flat:\\n\",flat)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例40：导入并显示CIFAR数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Downloading cifar-10-binary.tar.gz 100.0%\n",
      "Successfully down loaded cifar-10-binary.tar.gz 170052171 bytes.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#放在cifar目录下\n",
    "import cifar10\n",
    "\n",
    "cifar10.maybe_download_and_extract(data_dir=\"H:/tensorflow_projects/chap8/\")\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:209: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:65: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1241: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:113: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-1-053e7ad51f24>:26: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "__\n",
      " [[[ 1.2483675   0.04940185 -1.4983538 ]\n",
      "  [ 1.1175714   0.02760248 -1.563752  ]\n",
      "  [ 1.2483675   0.18019812 -1.4111564 ]\n",
      "  ...\n",
      "  [-0.60457945 -0.7353757  -0.9097707 ]\n",
      "  [-0.58278006 -0.8443726  -1.4111564 ]\n",
      "  [-0.14679253 -0.62637883 -1.4547552 ]]\n",
      "\n",
      " [[ 1.0739726   0.07120123 -1.4765545 ]\n",
      "  [ 1.16117     0.02760248 -1.5855514 ]\n",
      "  [ 1.2047688   0.09300061 -1.5419526 ]\n",
      "  ...\n",
      "  [-0.4519838  -0.4519838  -0.5609807 ]\n",
      "  [-1.1713632  -1.1713632  -1.4111564 ]\n",
      "  [-0.9751688  -1.1713632  -1.6291502 ]]\n",
      "\n",
      " [[ 0.89957756  0.11479998 -1.389357  ]\n",
      "  [ 1.270167    0.13659936 -1.4983538 ]\n",
      "  [ 1.2265682   0.07120123 -1.563752  ]\n",
      "  ...\n",
      "  [-0.58278006 -0.62637883 -0.7571751 ]\n",
      "  [-1.0623664  -0.88797134 -0.9315701 ]\n",
      "  [-1.4329557  -1.4111564  -1.5855514 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.35459313  0.79058063  1.2047688 ]\n",
      "  [ 0.35459313  0.5289881   0.68158376]\n",
      "  [ 2.251139    2.20754     2.1203427 ]\n",
      "  ...\n",
      "  [-1.0841657  -0.03779566  0.8777782 ]\n",
      "  [-1.0405669  -0.05959503  0.7687813 ]\n",
      "  [-1.1495639  -0.0813944   0.81238   ]]\n",
      "\n",
      " [[-0.9969682  -0.3647863   0.3109944 ]\n",
      "  [ 0.35459313  0.72518253  1.1393707 ]\n",
      "  [ 2.4473333   2.5563302   2.665327  ]\n",
      "  ...\n",
      "  [-1.4329557  -0.32118756  0.63798505]\n",
      "  [-1.4983538  -0.3647863   0.61618567]\n",
      "  [-1.6509495  -0.4519838   0.5725869 ]]\n",
      "\n",
      " [[-1.4765545  -0.62637883  0.3109944 ]\n",
      "  [-0.7353757  -0.12499316  0.5725869 ]\n",
      "  [ 1.967747    2.3383365   2.6435277 ]\n",
      "  ...\n",
      "  [-1.5855514  -0.40838507  0.5943863 ]\n",
      "  [-1.8253446  -0.58278006  0.46359   ]\n",
      "  [-1.563752   -0.23399004  0.89957756]]]\n",
      "__\n",
      " 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv0ikVPwikVLxi0RKxS8SKRW/SKT+F24a54wrGqqBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22d4c236438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#放在cifar目录下\n",
    "import cifar10_input\n",
    "import tensorflow as tf\n",
    "import pylab \n",
    "import numpy as np\n",
    "\n",
    "#取数据\n",
    "batch_size = 12\n",
    "data_dir = 'H:/tensorflow_projects/chap8/cifar-10-batches-bin'\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data = True, data_dir = data_dir, batch_size = batch_size)\n",
    "\n",
    "\n",
    "#sess = tf.InteractiveSession()\n",
    "#tf.global_variables_initializer().run()\n",
    "#tf.train.start_queue_runners()\n",
    "#image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "#print(\"__\\n\",image_batch[0])\n",
    "#\n",
    "#print(\"__\\n\",label_batch[0])\n",
    "#pylab.imshow(image_batch[0])\n",
    "#pylab.show()\n",
    "#\n",
    "\n",
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "print(\"__\\n\",image_batch[0])\n",
    "\n",
    "print(\"__\\n\",label_batch[0])\n",
    "pylab.imshow(  (image_batch[0]-np.min(image_batch[0]))  / (np.max(image_batch[0])-np.min(image_batch[0]) )   )\n",
    "pylab.show()\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "#    tf.global_variables_initializer().run()\n",
    "#    tf.train.start_queue_runners()\n",
    "#    image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "#    print(\"__\\n\",image_batch[0])\n",
    "#    \n",
    "#    print(\"__\\n\",label_batch[0])\n",
    "#    pylab.imshow(image_batch[0])\n",
    "#    pylab.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例41：显示CIFAR数据集的原始图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b5a44a5780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np  \n",
    "from scipy.misc import imsave  \n",
    "  \n",
    "filename = '../cifar-10-batches-bin/test_batch.bin'  \n",
    "  \n",
    "bytestream = open(filename, \"rb\")  \n",
    "buf = bytestream.read(10000 * (1 + 32 * 32 * 3))  \n",
    "bytestream.close()  \n",
    "  \n",
    "data = np.frombuffer(buf, dtype=np.uint8)  \n",
    "data = data.reshape(10000, 1 + 32*32*3)  \n",
    "labels_images = np.hsplit(data, [1])  \n",
    "labels = labels_images[0].reshape(10000)  \n",
    "images = labels_images[1].reshape(10000, 32, 32, 3)  \n",
    "  \n",
    "img = np.reshape(images[0], (3, 32, 32)) #导出第一幅图  \n",
    "img = img.transpose(1, 2, 0)  \n",
    "  \n",
    "import pylab \n",
    "print(labels[0]) \n",
    "pylab.imshow(img)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "2.0\n",
      "-------------------------\n",
      "3.0\n",
      "-------------------------\n",
      "3.0\n",
      "-------------------------\n",
      "4.0\n",
      "-------------------------\n",
      "5.0\n",
      "-------------------------\n",
      "6.0\n",
      "-------------------------\n",
      "7.0\n",
      "-------------------------\n",
      "7.0\n",
      "-------------------------\n",
      "9.0\n",
      "-------------------------\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf  \n",
    "\n",
    "#创建长度为100的队列  \n",
    "queue = tf.FIFOQueue(100,\"float\")  \n",
    "\n",
    "c = tf.Variable(0.0)  #计数器  \n",
    "#加1操作 \n",
    "op = tf.assign_add(c,tf.constant(1.0))  \n",
    "#操作:将计数器的结果加入队列  \n",
    "enqueue_op = queue.enqueue(c)  \n",
    "  \n",
    "#创建一个队列管理器QueueRunner，用这两个操作向q中添加元素。目前我们只使用一个线程:  \n",
    "qr = tf.train.QueueRunner(queue,enqueue_ops=[op,enqueue_op]) \n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())  \n",
    "       \n",
    "    coord = tf.train.Coordinator()  \n",
    "      \n",
    "    ## 启动入队线程, Coordinator是线程的参数  \n",
    "    enqueue_threads = qr.create_threads(sess, coord = coord,start=True)  # 启动入队线程  \n",
    "      \n",
    "    # 主线程  \n",
    "    for i in range(0, 10):  \n",
    "        print (\"-------------------------\")  \n",
    "        print(sess.run(queue.dequeue()))  \n",
    "      \n",
    "     \n",
    "    coord.request_stop()  #通知其他线程关闭 其他所有线程关闭之后，这一函数才能返回  \n",
    "\n",
    "\n",
    "    #join操作经常用在线程当中,其作用是等待某线程结束  \n",
    "    #coord.join(enqueue_threads) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例42：协调器的用法演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__\n",
      " [[[ 1.0754309   1.0754309   1.0754309 ]\n",
      "  [ 1.0754309   1.0754309   1.0754309 ]\n",
      "  [ 1.0754309   1.0754309   1.0754309 ]\n",
      "  ...\n",
      "  [ 1.0754309   1.0754309   1.0754309 ]\n",
      "  [ 1.0754309   1.0754309   1.0754309 ]\n",
      "  [ 1.0754309   1.0754309   1.0754309 ]]\n",
      "\n",
      " [[ 1.088781    1.0754309   1.0754309 ]\n",
      "  [ 1.088781    1.0754309   1.0754309 ]\n",
      "  [ 1.0754309   1.088781    1.0754309 ]\n",
      "  ...\n",
      "  [ 1.088781    1.088781    1.0754309 ]\n",
      "  [ 1.088781    1.088781    1.0754309 ]\n",
      "  [ 1.088781    1.088781    1.0754309 ]]\n",
      "\n",
      " [[ 1.0620806   1.0353804   1.0487305 ]\n",
      "  [ 1.1021312   1.088781    1.088781  ]\n",
      "  [ 1.1021312   1.1021312   1.0754309 ]\n",
      "  ...\n",
      "  [ 1.088781    1.1021312   1.0620806 ]\n",
      "  [ 1.0754309   1.088781    1.0487305 ]\n",
      "  [ 1.0754309   1.0754309   1.0754309 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.9016584  -1.9817594  -1.9684093 ]\n",
      "  [-1.6613551  -1.9016584  -1.8349075 ]\n",
      "  [-1.6346549  -1.9016584  -1.8349075 ]\n",
      "  ...\n",
      "  [-0.6734422  -0.82029414 -0.887045  ]\n",
      "  [-0.43313903 -0.48653972 -0.5532906 ]\n",
      "  [-0.3396878  -0.29963726 -0.3797383 ]]\n",
      "\n",
      " [[-2.0084598  -2.0485103  -2.0618606 ]\n",
      "  [-1.9016584  -2.03516    -1.9951096 ]\n",
      "  [-1.8215573  -2.02181    -1.9417089 ]\n",
      "  ...\n",
      "  [-0.3930885  -0.3797383  -0.5132401 ]\n",
      "  [-0.3930885  -0.35303795 -0.45983937]\n",
      "  [-0.3396878  -0.25958672 -0.35303795]]\n",
      "\n",
      " [[-2.03516    -2.0618606  -2.0618606 ]\n",
      "  [-1.9951096  -2.0485103  -2.03516   ]\n",
      "  [-1.9684093  -2.0485103  -1.9951096 ]\n",
      "  ...\n",
      "  [-0.43313903 -0.3797383  -0.5265902 ]\n",
      "  [-0.44648919 -0.36638814 -0.48653972]\n",
      "  [-0.29963726 -0.20618603 -0.31298745]]]\n",
      "__\n",
      " 8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Floating point image RGB values must be in the 0..1 range.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    330\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'png'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'retina'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'png2x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[0;32m   2214\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2215\u001b[0m                     \u001b[0mdryrun\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2216\u001b[1;33m                     **kwargs)\n\u001b[0m\u001b[0;32m   2217\u001b[0m                 \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2218\u001b[0m                 \u001b[0mbbox_inches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[1;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprint_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[0moriginal_dpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;31m# if toolbar:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;31m#     toolbar.set_cursor(cursors.WAIT)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;31m# if toolbar:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[1;32m-> 1299\u001b[1;33m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'figure'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, inframe)\u001b[0m\n\u001b[0;32m   2435\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2437\u001b[1;33m         \u001b[0mmimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2439\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'axes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[1;32m--> 566\u001b[1;33m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mmake_image\u001b[1;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[0;32m    791\u001b[0m         return self._make_image(\n\u001b[0;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagnification\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m             unsampled=unsampled)\n\u001b[0m\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_make_image\u001b[1;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;31m# (of int or float)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m             \u001b[1;31m# or an RGBA array of re-sampled input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m             \u001b[1;31m# output is now a correctly sized RGBA array of uint8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\cm.py\u001b[0m in \u001b[0;36mto_rgba\u001b[1;34m(self, x, alpha, bytes, norm)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'f'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                         raise ValueError(\"Floating point image RGB values \"\n\u001b[0m\u001b[0;32m    258\u001b[0m                                          \"must be in the 0..1 range.\")\n\u001b[0;32m    259\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Floating point image RGB values must be in the 0..1 range."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b64e15b048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#放在cifar目录下\n",
    "import  cifar10_input\n",
    "import tensorflow as tf\n",
    "import pylab \n",
    "\n",
    "#取数据\n",
    "batch_size = 12\n",
    "data_dir = '../cifar-10-batches-bin'\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data = True, data_dir = data_dir, batch_size = batch_size)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess, coord)\n",
    "    \n",
    "    image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "    print(\"__\\n\",image_batch[0])\n",
    "    \n",
    "    print(\"__\\n\",label_batch[0])\n",
    "    pylab.imshow(image_batch[0])\n",
    "    pylab.show()\n",
    "    coord.request_stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例44：建立一个带有全局平均池化层的卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "begin data\n",
      "step 0, training accuracy 0.164062\n",
      "step 200, training accuracy 0.320312\n",
      "step 400, training accuracy 0.414062\n",
      "step 600, training accuracy 0.4375\n",
      "step 800, training accuracy 0.429688\n",
      "step 1000, training accuracy 0.507812\n",
      "step 1200, training accuracy 0.367188\n",
      "step 1400, training accuracy 0.523438\n",
      "step 1600, training accuracy 0.507812\n",
      "step 1800, training accuracy 0.445312\n",
      "step 2000, training accuracy 0.5625\n",
      "step 2200, training accuracy 0.5\n",
      "step 2400, training accuracy 0.507812\n",
      "step 2600, training accuracy 0.460938\n",
      "step 2800, training accuracy 0.585938\n",
      "step 3000, training accuracy 0.617188\n",
      "step 3200, training accuracy 0.554688\n",
      "step 3400, training accuracy 0.546875\n",
      "step 3600, training accuracy 0.554688\n",
      "step 3800, training accuracy 0.523438\n",
      "step 4000, training accuracy 0.554688\n",
      "step 4200, training accuracy 0.59375\n",
      "step 4400, training accuracy 0.546875\n",
      "step 4600, training accuracy 0.601562\n",
      "step 4800, training accuracy 0.625\n",
      "step 5000, training accuracy 0.554688\n",
      "step 5200, training accuracy 0.632812\n",
      "step 5400, training accuracy 0.617188\n",
      "step 5600, training accuracy 0.5625\n",
      "step 5800, training accuracy 0.59375\n",
      "step 6000, training accuracy 0.515625\n",
      "step 6200, training accuracy 0.6875\n",
      "step 6400, training accuracy 0.578125\n",
      "step 6600, training accuracy 0.578125\n",
      "step 6800, training accuracy 0.601562\n",
      "step 7000, training accuracy 0.640625\n",
      "step 7200, training accuracy 0.640625\n",
      "step 7400, training accuracy 0.609375\n",
      "step 7600, training accuracy 0.617188\n",
      "step 7800, training accuracy 0.609375\n",
      "step 8000, training accuracy 0.640625\n",
      "step 8200, training accuracy 0.578125\n",
      "step 8400, training accuracy 0.609375\n",
      "step 8600, training accuracy 0.59375\n",
      "step 8800, training accuracy 0.625\n",
      "step 9000, training accuracy 0.617188\n",
      "step 9200, training accuracy 0.648438\n",
      "step 9400, training accuracy 0.617188\n",
      "step 9600, training accuracy 0.648438\n",
      "step 9800, training accuracy 0.617188\n",
      "step 10000, training accuracy 0.632812\n",
      "step 10200, training accuracy 0.65625\n",
      "step 10400, training accuracy 0.679688\n",
      "step 10600, training accuracy 0.609375\n",
      "step 10800, training accuracy 0.664062\n",
      "step 11000, training accuracy 0.734375\n",
      "step 11200, training accuracy 0.632812\n",
      "step 11400, training accuracy 0.6875\n",
      "step 11600, training accuracy 0.71875\n",
      "step 11800, training accuracy 0.585938\n",
      "step 12000, training accuracy 0.679688\n",
      "step 12200, training accuracy 0.671875\n",
      "step 12400, training accuracy 0.640625\n",
      "step 12600, training accuracy 0.609375\n",
      "step 12800, training accuracy 0.679688\n",
      "step 13000, training accuracy 0.664062\n",
      "step 13200, training accuracy 0.5625\n",
      "step 13400, training accuracy 0.625\n",
      "step 13600, training accuracy 0.65625\n",
      "step 13800, training accuracy 0.640625\n",
      "step 14000, training accuracy 0.6875\n",
      "step 14200, training accuracy 0.664062\n",
      "step 14400, training accuracy 0.695312\n",
      "step 14600, training accuracy 0.703125\n",
      "step 14800, training accuracy 0.671875\n",
      "finished！ test accuracy 0.664062\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 1. 引入数据集\n",
    "batch_size = 128\n",
    "data_dir = '../cifar-10-batches-bin'\n",
    "print(\"begin\")\n",
    "images_train, labels_train = cifar10_input.inputs(\n",
    "    eval_data=False, data_dir=data_dir, batch_size=batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(\n",
    "    eval_data=True, data_dir=data_dir, batch_size=batch_size)\n",
    "print(\"begin data\")\n",
    "\n",
    "# 2. 定义网络结构\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "\n",
    "\n",
    "return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def avg_pool_6x6(x):\n",
    "\n",
    "\n",
    "return tf.nn.avg_pool(x, ksize=[1, 6, 6, 1],\n",
    "                      strides=[1, 6, 6, 1], padding='SAME')\n",
    "\n",
    "# tf Graph Input\n",
    "# cifar data image of shape 24*24*3\n",
    "x = tf.placeholder(tf.float32, [None, 24, 24, 3])\n",
    "y = tf.placeholder(tf.float32, [None, 10])  # 0-9 数字=> 10 classes\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 24, 24, 3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 64, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3 = avg_pool_6x6(h_conv3)  # 10\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv = tf.nn.softmax(nt_hpool3_flat)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# 3. 运行session进行训练\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "for i in range(15000):\n",
    "    # 20000\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    label_b = np.eye(10, dtype=float)[label_batch]  # one hot\n",
    "\n",
    "    train_step.run(feed_dict={x: image_batch, y: label_b}, session=sess)\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        train_accuracy = accuracy.eval(\n",
    "            feed_dict={x: image_batch, y: label_b}, session=sess)\n",
    "    print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "\n",
    "# 4. 评估结果\n",
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "label_b = np.eye(10, dtype=float)[label_batch]  # one hot\n",
    "print(\"finished！ test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "    x: image_batch, y: label_b}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST卷积和CIFAR卷积分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-20be67baac2c>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "step 0, training accuracy 0.26\n",
      "step 20, training accuracy 0.14\n",
      "step 40, training accuracy 0.14\n",
      "step 60, training accuracy 0.26\n",
      "step 80, training accuracy 0.18\n",
      "step 100, training accuracy 0.22\n",
      "step 120, training accuracy 0.22\n",
      "step 140, training accuracy 0.3\n",
      "step 160, training accuracy 0.12\n",
      "step 180, training accuracy 0.34\n",
      "test accuracy 0.2344\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "# 导入 MINST 数据集\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/data/\", one_hot=True)\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')  \n",
    "def avg_pool_7x7(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 7, 7, 1],\n",
    "                        strides=[1, 7, 7, 1], padding='SAME')\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data维度 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 数字=> 10 classes\n",
    "\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "#########################################################new\n",
    "W_conv3 = weight_variable([5, 5, 64, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3=avg_pool_7x7(h_conv3)#64\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv=tf.nn.softmax(nt_hpool3_flat)\n",
    "\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# 启动session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(200):#20000\n",
    "        batch = mnist.train.next_batch(50)#50\n",
    "        if i%20 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:batch[0], y: batch[1]})\n",
    "            print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1]})\n",
    "    \n",
    "    print (\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:209: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:65: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1241: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:113: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "begin data\n",
      "WARNING:tensorflow:From <ipython-input-1-7259015b4d97>:63: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-7259015b4d97>:87: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "step 0, training accuracy 0.117188\n",
      "step 200, training accuracy 0.3125\n",
      "step 400, training accuracy 0.34375\n",
      "step 600, training accuracy 0.320312\n",
      "step 800, training accuracy 0.390625\n",
      "step 1000, training accuracy 0.289062\n",
      "step 1200, training accuracy 0.40625\n",
      "step 1400, training accuracy 0.328125\n",
      "step 1600, training accuracy 0.515625\n",
      "step 1800, training accuracy 0.460938\n",
      "step 2000, training accuracy 0.453125\n",
      "step 2200, training accuracy 0.5625\n",
      "step 2400, training accuracy 0.484375\n",
      "step 2600, training accuracy 0.5\n",
      "step 2800, training accuracy 0.5625\n",
      "step 3000, training accuracy 0.53125\n",
      "step 3200, training accuracy 0.421875\n",
      "step 3400, training accuracy 0.476562\n",
      "step 3600, training accuracy 0.507812\n",
      "step 3800, training accuracy 0.578125\n",
      "step 4000, training accuracy 0.53125\n",
      "step 4200, training accuracy 0.507812\n",
      "step 4400, training accuracy 0.515625\n",
      "step 4600, training accuracy 0.570312\n",
      "step 4800, training accuracy 0.546875\n",
      "step 5000, training accuracy 0.617188\n",
      "step 5200, training accuracy 0.585938\n",
      "step 5400, training accuracy 0.59375\n",
      "step 5600, training accuracy 0.492188\n",
      "step 5800, training accuracy 0.664062\n",
      "step 6000, training accuracy 0.523438\n",
      "step 6200, training accuracy 0.59375\n",
      "step 6400, training accuracy 0.648438\n",
      "step 6600, training accuracy 0.609375\n",
      "step 6800, training accuracy 0.632812\n",
      "step 7000, training accuracy 0.679688\n",
      "step 7200, training accuracy 0.640625\n",
      "step 7400, training accuracy 0.664062\n",
      "step 7600, training accuracy 0.539062\n",
      "step 7800, training accuracy 0.65625\n",
      "step 8000, training accuracy 0.726562\n",
      "step 8200, training accuracy 0.570312\n",
      "step 8400, training accuracy 0.671875\n",
      "step 8600, training accuracy 0.71875\n",
      "step 8800, training accuracy 0.703125\n",
      "step 9000, training accuracy 0.671875\n",
      "step 9200, training accuracy 0.632812\n",
      "step 9400, training accuracy 0.75\n",
      "step 9600, training accuracy 0.734375\n",
      "step 9800, training accuracy 0.664062\n",
      "step 10000, training accuracy 0.703125\n",
      "step 10200, training accuracy 0.734375\n",
      "step 10400, training accuracy 0.734375\n",
      "step 10600, training accuracy 0.695312\n",
      "step 10800, training accuracy 0.789062\n",
      "step 11000, training accuracy 0.765625\n",
      "step 11200, training accuracy 0.726562\n",
      "step 11400, training accuracy 0.765625\n",
      "step 11600, training accuracy 0.734375\n",
      "step 11800, training accuracy 0.734375\n",
      "step 12000, training accuracy 0.78125\n",
      "step 12200, training accuracy 0.796875\n",
      "step 12400, training accuracy 0.757812\n",
      "step 12600, training accuracy 0.671875\n",
      "step 12800, training accuracy 0.8125\n",
      "step 13000, training accuracy 0.851562\n",
      "step 13200, training accuracy 0.773438\n",
      "step 13400, training accuracy 0.75\n",
      "step 13600, training accuracy 0.796875\n",
      "step 13800, training accuracy 0.8125\n",
      "step 14000, training accuracy 0.742188\n",
      "step 14200, training accuracy 0.765625\n",
      "step 14400, training accuracy 0.742188\n",
      "step 14600, training accuracy 0.820312\n",
      "step 14800, training accuracy 0.789062\n",
      "finished！ test accuracy 0.671875\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import  cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "data_dir = '../cifar-10-batches-bin'\n",
    "print(\"begin\")\n",
    "images_train, labels_train = cifar10_input.inputs(eval_data = False,data_dir = data_dir, batch_size = batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data = True, data_dir = data_dir, batch_size = batch_size)\n",
    "print(\"begin data\")\n",
    "\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')  \n",
    "                        \n",
    "def avg_pool_6x6(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 6, 6, 1],\n",
    "                        strides=[1, 6, 6, 1], padding='SAME')\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 24,24,3]) # mnist data维度 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 数字=> 10 classes\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,24,24,3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "W_fc1 = weight_variable([6 * 6 * 64, 256])\n",
    "b_fc1 = bias_variable([256])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 6*6*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([256, 128])\n",
    "b_fc2 = bias_variable([128])\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "W_fc3 = weight_variable([128, 10])\n",
    "b_fc3 = bias_variable([10])\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc2, W_fc3) + b_fc3)\n",
    "\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))\n",
    "\n",
    "#不同的优化方法测测效果\n",
    "#train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(cross_entropy)\n",
    "#train_step = tf.train.AdagradOptimizer(1e-5).minimize(cross_entropy)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "for i in range(15000):#20000\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    label_b = np.eye(10,dtype=float)[label_batch] #one hot\n",
    "  \n",
    "    train_step.run(feed_dict={x:image_batch, y: label_b, keep_prob: 0.5},session=sess)\n",
    "  \n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:image_batch, y: label_b, keep_prob: 1.0},session=sess)\n",
    "        print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "\n",
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "label_b = np.eye(10,dtype=float)[label_batch]#one hot\n",
    "print (\"finished！ test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "     x:image_batch, y: label_b, keep_prob: 1.0},session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例45：演示反卷积操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "(1, 2, 2, 1)\n",
      "conv:\n",
      " [array([[[[-2.],\n",
      "         [-2.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [-2.]]]], dtype=float32), array([[[[ 1.]],\n",
      "\n",
      "        [[ 0.]]],\n",
      "\n",
      "\n",
      "       [[[-1.]],\n",
      "\n",
      "        [[-2.]]]], dtype=float32)]\n",
      "cons:\n",
      " [array([[[[-2.],\n",
      "         [-2.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [-2.]]]], dtype=float32)]\n",
      "contv:\n",
      " [array([[[[-2.],\n",
      "         [ 0.],\n",
      "         [-2.],\n",
      "         [ 0.]],\n",
      "\n",
      "        [[ 2.],\n",
      "         [ 4.],\n",
      "         [ 2.],\n",
      "         [ 4.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [ 0.],\n",
      "         [-2.],\n",
      "         [ 0.]],\n",
      "\n",
      "        [[ 2.],\n",
      "         [ 4.],\n",
      "         [ 2.],\n",
      "         [ 4.]]]], dtype=float32)]\n",
      "conts:\n",
      " [array([[[[-2.],\n",
      "         [ 0.],\n",
      "         [-2.],\n",
      "         [ 0.]],\n",
      "\n",
      "        [[ 2.],\n",
      "         [ 4.],\n",
      "         [ 2.],\n",
      "         [ 4.]],\n",
      "\n",
      "        [[-2.],\n",
      "         [ 0.],\n",
      "         [-2.],\n",
      "         [ 0.]],\n",
      "\n",
      "        [[ 2.],\n",
      "         [ 4.],\n",
      "         [ 2.],\n",
      "         [ 4.]]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "img = tf.Variable(tf.constant(1.0,shape = [1, 4, 4, 1])) \n",
    "\n",
    "filter =  tf.Variable(tf.constant([1.0,0,-1,-2],shape = [2, 2, 1, 1]))\n",
    "\n",
    "conv = tf.nn.conv2d(img, filter, strides=[1, 2, 2, 1], padding='VALID')  \n",
    "cons = tf.nn.conv2d(img, filter, strides=[1, 2, 2, 1], padding='SAME')\n",
    "print(conv.shape)\n",
    "print(cons.shape)\n",
    " \n",
    "contv= tf.nn.conv2d_transpose(conv, filter, [1,4,4,1],strides=[1, 2, 2, 1], padding='VALID')\n",
    "conts = tf.nn.conv2d_transpose(cons, filter, [1,4,4,1],strides=[1, 2, 2, 1], padding='SAME')\n",
    " \n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer() )  \n",
    "\n",
    "    print(\"conv:\\n\",sess.run([conv,filter])) \n",
    "    print(\"cons:\\n\",sess.run([cons]))    \n",
    "    print(\"contv:\\n\",sess.run([contv])) \n",
    "    print(\"conts:\\n\",sess.run([conts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例46：演示反池化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 2)\n",
      "(1, 2, 2, 2)\n",
      "(1, 2, 2, 2)\n",
      "(1, 4, 4, 2)\n",
      "image:\n",
      "[[[[0. 4.]\n",
      "   [0. 4.]\n",
      "   [0. 4.]\n",
      "   [0. 4.]]\n",
      "\n",
      "  [[1. 5.]\n",
      "   [1. 5.]\n",
      "   [1. 5.]\n",
      "   [1. 5.]]\n",
      "\n",
      "  [[2. 6.]\n",
      "   [2. 6.]\n",
      "   [2. 6.]\n",
      "   [2. 6.]]\n",
      "\n",
      "  [[3. 7.]\n",
      "   [3. 7.]\n",
      "   [3. 7.]\n",
      "   [3. 7.]]]]\n",
      "pooling2:\n",
      " [[[[1. 5.]\n",
      "   [1. 5.]]\n",
      "\n",
      "  [[3. 7.]\n",
      "   [3. 7.]]]]\n",
      "encode:\n",
      " [[[[1. 5.]\n",
      "   [1. 5.]]\n",
      "\n",
      "  [[3. 7.]\n",
      "   [3. 7.]]]] [[[[ 8  9]\n",
      "   [12 13]]\n",
      "\n",
      "  [[24 25]\n",
      "   [28 29]]]]\n",
      "reslut:\n",
      " [[[[0. 0.]\n",
      "   [0. 0.]\n",
      "   [0. 0.]\n",
      "   [0. 0.]]\n",
      "\n",
      "  [[1. 5.]\n",
      "   [0. 0.]\n",
      "   [1. 5.]\n",
      "   [0. 0.]]\n",
      "\n",
      "  [[0. 0.]\n",
      "   [0. 0.]\n",
      "   [0. 0.]\n",
      "   [0. 0.]]\n",
      "\n",
      "  [[3. 7.]\n",
      "   [0. 0.]\n",
      "   [3. 7.]\n",
      "   [0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def max_pool_with_argmax(net, stride):\n",
    "    _, mask = tf.nn.max_pool_with_argmax(net, ksize=[1, stride, stride, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "    mask = tf.stop_gradient(mask)\n",
    "    net = tf.nn.max_pool(net, ksize=[1, stride, stride, 1],strides=[1, stride, stride, 1], padding='SAME') \n",
    "    return net, mask\n",
    " \n",
    "\n",
    "def unpool(net, mask, stride):\n",
    "    ksize = [1, stride, stride, 1]\n",
    "    input_shape = net.get_shape().as_list()\n",
    "    #  calculation new shape\n",
    "    output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n",
    "    # calculation indices for batch, height, width and feature maps\n",
    "    one_like_mask = tf.ones_like(mask)\n",
    "    batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int64), shape=[input_shape[0], 1, 1, 1])\n",
    "    b = one_like_mask * batch_range\n",
    "    y = mask // (output_shape[2] * output_shape[3])\n",
    "    x = mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n",
    "    feature_range = tf.range(output_shape[3], dtype=tf.int64)\n",
    "    f = one_like_mask * feature_range\n",
    "    # transpose indices & reshape update values to one dimension\n",
    "    updates_size = tf.size(net)\n",
    "    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n",
    "    values = tf.reshape(net, [updates_size])\n",
    "    ret = tf.scatter_nd(indices, values, output_shape)\n",
    "    return ret\n",
    "    \n",
    "    \n",
    "img=tf.constant([  \n",
    "        [[0.0,4.0],[0.0,4.0],[0.0,4.0],[0.0,4.0]],  \n",
    "        [[1.0,5.0],[1.0,5.0],[1.0,5.0],[1.0,5.0]],  \n",
    "        [[2.0,6.0],[2.0,6.0],[2.0,6.0],[2.0,6.0]],  \n",
    "        [[3.0,7.0],[3.0,7.0], [3.0,7.0],[3.0,7.0]]\n",
    "    ])  \n",
    "  \n",
    "img=tf.reshape(img,[1,4,4,2])  \n",
    "pooling2=tf.nn.max_pool(img,[1,2,2,1],[1,2,2,1],padding='SAME')  \n",
    "encode, mask = max_pool_with_argmax(img, 2)\n",
    "img2 = unpool(encode,mask,2)\n",
    "print(img.shape)\n",
    "print(encode.shape)\n",
    "print(mask.shape)\n",
    "print(img2.shape)\n",
    "with tf.Session() as sess:  \n",
    "    print(\"image:\")  \n",
    "    print (sess.run(img))     \n",
    "    result=sess.run(pooling2)  \n",
    "    print (\"pooling2:\\n\",result)\n",
    "    result,mask2=sess.run([encode, mask])  \n",
    "    print (\"encode:\\n\",result,mask2)\n",
    "    result=sess.run(img2)  \n",
    "    print (\"reslut:\\n\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例47：演示gradients的基本用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 9., 10.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "w1 = tf.Variable([[1.,2]])\n",
    "w2 = tf.Variable([[3.,4]])\n",
    "\n",
    "y = tf.matmul(w1, [[9.],[10]])\n",
    "#grads = tf.gradients(y,[w1,w2])#w2不相干，会报错\n",
    "grads = tf.gradients(y,[w1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    gradval = sess.run(grads)\n",
    "    print(gradval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例48：使用gradients对多个式子求多变量求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2.], dtype=float32), array([1., 2.], dtype=float32), array([4., 6.], dtype=float32), array([3., 4.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "w1 = tf.get_variable('w1', shape=[2])\n",
    "w2 = tf.get_variable('w2', shape=[2])\n",
    "\n",
    "w3 = tf.get_variable('w3', shape=[2])\n",
    "w4 = tf.get_variable('w4', shape=[2])\n",
    "\n",
    "y1 = w1 + w2+ w3\n",
    "y2 = w3 + w4\n",
    "\n",
    "a = w1+w2\n",
    "a_stoped = tf.stop_gradient(a)\n",
    "y3= a_stoped+w3\n",
    "\n",
    "gradients = tf.gradients([y1, y2], [w1, w2, w3, w4], grad_ys=[tf.convert_to_tensor([1.,2.]),\n",
    "                                                          tf.convert_to_tensor([3.,4.])])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(gradients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例49：演示gradients停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, <tf.Tensor 'gradients_1/grad_ys_0:0' shape=(2,) dtype=float32>]\n",
      "[array([1., 2.], dtype=float32), array([1., 2.], dtype=float32), array([4., 6.], dtype=float32), array([3., 4.], dtype=float32)]\n",
      "[array([1., 2.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "w1 = tf.get_variable('w1', shape=[2])\n",
    "w2 = tf.get_variable('w2', shape=[2])\n",
    "\n",
    "w3 = tf.get_variable('w3', shape=[2])\n",
    "w4 = tf.get_variable('w4', shape=[2])\n",
    "\n",
    "y1 = w1 + w2+ w3\n",
    "y2 = w3 + w4\n",
    "\n",
    "a = w1+w2\n",
    "a_stoped = tf.stop_gradient(a)\n",
    "y3= a_stoped+w3\n",
    "\n",
    "gradients = tf.gradients([y1, y2], [w1, w2, w3, w4], grad_ys=[tf.convert_to_tensor([1.,2.]),\n",
    "                                                          tf.convert_to_tensor([3.,4.])])\n",
    "                                                          \n",
    "gradients2 = tf.gradients(y3, [w1, w2, w3], grad_ys=tf.convert_to_tensor([1.,2.]))                                                          \n",
    "print(gradients2) \n",
    " \n",
    "gradients3 = tf.gradients(y3, [ w3], grad_ys=tf.convert_to_tensor([1.,2.])) \n",
    "                                                       \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(gradients))\n",
    "    #print(sess.run(gradients2))#报错\n",
    "    print(sess.run(gradients3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例50：用反卷积复原CNN各层图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:209: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:65: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1241: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:113: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "begin data\n",
      "(128, 6, 6, 64)\n",
      "(128, 12, 12, 64) (128, 12, 12, 64) (128, 12, 12, 64)\n",
      "WARNING:tensorflow:From <ipython-input-1-cb0d3a89a025>:123: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "step 0, training accuracy 0.109375\n",
      "cross_entropy 299.33258\n",
      "step 200, training accuracy 0.265625\n",
      "cross_entropy 255.26352\n",
      "step 400, training accuracy 0.328125\n",
      "cross_entropy 220.17691\n",
      "step 600, training accuracy 0.304688\n",
      "cross_entropy 239.9032\n",
      "step 800, training accuracy 0.4375\n",
      "cross_entropy 215.55\n",
      "step 1000, training accuracy 0.34375\n",
      "cross_entropy 240.33012\n",
      "step 1200, training accuracy 0.507812\n",
      "cross_entropy 208.6071\n",
      "step 1400, training accuracy 0.414062\n",
      "cross_entropy 215.67905\n",
      "step 1600, training accuracy 0.484375\n",
      "cross_entropy 213.92612\n",
      "step 1800, training accuracy 0.335938\n",
      "cross_entropy 233.1239\n",
      "step 2000, training accuracy 0.5\n",
      "cross_entropy 208.82675\n",
      "step 2200, training accuracy 0.492188\n",
      "cross_entropy 191.40732\n",
      "step 2400, training accuracy 0.460938\n",
      "cross_entropy 193.65083\n",
      "step 2600, training accuracy 0.492188\n",
      "cross_entropy 186.18613\n",
      "step 2800, training accuracy 0.484375\n",
      "cross_entropy 189.64601\n",
      "step 3000, training accuracy 0.539062\n",
      "cross_entropy 183.80658\n",
      "step 3200, training accuracy 0.421875\n",
      "cross_entropy 204.8297\n",
      "step 3400, training accuracy 0.476562\n",
      "cross_entropy 193.53462\n",
      "step 3600, training accuracy 0.515625\n",
      "cross_entropy 204.49474\n",
      "step 3800, training accuracy 0.523438\n",
      "cross_entropy 194.06113\n",
      "step 4000, training accuracy 0.53125\n",
      "cross_entropy 194.76062\n",
      "step 4200, training accuracy 0.476562\n",
      "cross_entropy 191.30899\n",
      "step 4400, training accuracy 0.484375\n",
      "cross_entropy 198.17056\n",
      "step 4600, training accuracy 0.523438\n",
      "cross_entropy 176.11752\n",
      "step 4800, training accuracy 0.5\n",
      "cross_entropy 185.19417\n",
      "step 5000, training accuracy 0.570312\n",
      "cross_entropy 188.2881\n",
      "step 5200, training accuracy 0.515625\n",
      "cross_entropy 184.50743\n",
      "step 5400, training accuracy 0.632812\n",
      "cross_entropy 175.47253\n",
      "step 5600, training accuracy 0.515625\n",
      "cross_entropy 193.34174\n",
      "step 5800, training accuracy 0.554688\n",
      "cross_entropy 181.11494\n",
      "step 6000, training accuracy 0.53125\n",
      "cross_entropy 172.79439\n",
      "step 6200, training accuracy 0.65625\n",
      "cross_entropy 153.55139\n",
      "step 6400, training accuracy 0.523438\n",
      "cross_entropy 183.32246\n",
      "step 6600, training accuracy 0.570312\n",
      "cross_entropy 166.75311\n",
      "step 6800, training accuracy 0.539062\n",
      "cross_entropy 192.8255\n",
      "step 7000, training accuracy 0.585938\n",
      "cross_entropy 172.40381\n",
      "step 7200, training accuracy 0.625\n",
      "cross_entropy 154.44427\n",
      "step 7400, training accuracy 0.523438\n",
      "cross_entropy 173.51205\n",
      "step 7600, training accuracy 0.617188\n",
      "cross_entropy 158.45212\n",
      "step 7800, training accuracy 0.515625\n",
      "cross_entropy 194.20056\n",
      "step 8000, training accuracy 0.59375\n",
      "cross_entropy 177.66724\n",
      "step 8200, training accuracy 0.476562\n",
      "cross_entropy 190.3621\n",
      "step 8400, training accuracy 0.59375\n",
      "cross_entropy 173.1125\n",
      "step 8600, training accuracy 0.53125\n",
      "cross_entropy 181.5401\n",
      "step 8800, training accuracy 0.570312\n",
      "cross_entropy 160.98604\n",
      "step 9000, training accuracy 0.59375\n",
      "cross_entropy 169.50435\n",
      "step 9200, training accuracy 0.65625\n",
      "cross_entropy 159.84276\n",
      "step 9400, training accuracy 0.53125\n",
      "cross_entropy 185.61572\n",
      "step 9600, training accuracy 0.609375\n",
      "cross_entropy 163.57227\n",
      "step 9800, training accuracy 0.539062\n",
      "cross_entropy 172.54062\n",
      "step 10000, training accuracy 0.570312\n",
      "cross_entropy 182.86372\n",
      "step 10200, training accuracy 0.5625\n",
      "cross_entropy 175.23633\n",
      "step 10400, training accuracy 0.625\n",
      "cross_entropy 158.63727\n",
      "step 10600, training accuracy 0.570312\n",
      "cross_entropy 183.32611\n",
      "step 10800, training accuracy 0.539062\n",
      "cross_entropy 186.68808\n",
      "step 11000, training accuracy 0.554688\n",
      "cross_entropy 180.54099\n",
      "step 11200, training accuracy 0.523438\n",
      "cross_entropy 187.641\n",
      "step 11400, training accuracy 0.601562\n",
      "cross_entropy 157.28458\n",
      "step 11600, training accuracy 0.5625\n",
      "cross_entropy 177.28255\n",
      "step 11800, training accuracy 0.601562\n",
      "cross_entropy 180.65999\n",
      "step 12000, training accuracy 0.601562\n",
      "cross_entropy 155.04745\n",
      "step 12200, training accuracy 0.625\n",
      "cross_entropy 164.97495\n",
      "step 12400, training accuracy 0.554688\n",
      "cross_entropy 160.26328\n",
      "step 12600, training accuracy 0.609375\n",
      "cross_entropy 169.59125\n",
      "step 12800, training accuracy 0.578125\n",
      "cross_entropy 181.78036\n",
      "step 13000, training accuracy 0.65625\n",
      "cross_entropy 155.21802\n",
      "step 13200, training accuracy 0.507812\n",
      "cross_entropy 183.50308\n",
      "step 13400, training accuracy 0.617188\n",
      "cross_entropy 162.36247\n",
      "step 13600, training accuracy 0.585938\n",
      "cross_entropy 172.93718\n",
      "step 13800, training accuracy 0.617188\n",
      "cross_entropy 167.09555\n",
      "step 14000, training accuracy 0.664062\n",
      "cross_entropy 153.10864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14200, training accuracy 0.648438\n",
      "cross_entropy 154.17062\n",
      "step 14400, training accuracy 0.546875\n",
      "cross_entropy 185.39635\n",
      "step 14600, training accuracy 0.640625\n",
      "cross_entropy 152.71246\n",
      "step 14800, training accuracy 0.617188\n",
      "cross_entropy 162.1098\n",
      "finished！ test accuracy 0.601562\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#最大池化\n",
    "def max_pool_with_argmax(net, stride):\n",
    "    _, mask = tf.nn.max_pool_with_argmax( net,ksize=[1, stride, stride, 1], strides=[1, stride, stride, 1],padding='SAME')\n",
    "    mask = tf.stop_gradient(mask)\n",
    "    net = tf.nn.max_pool(net, ksize=[1, stride, stride, 1],strides=[1, stride, stride, 1], padding='SAME') \n",
    "    return net, mask\n",
    "#4*4----2*2--=2*2 【6，8，12，16】    \n",
    "#反池化\n",
    "def unpool(net, mask, stride):\n",
    "    ksize = [1, stride, stride, 1]\n",
    "    input_shape = net.get_shape().as_list()\n",
    "\n",
    "    output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n",
    "\n",
    "    one_like_mask = tf.ones_like(mask)\n",
    "    batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int64), shape=[input_shape[0], 1, 1, 1])\n",
    "    b = one_like_mask * batch_range\n",
    "    y = mask // (output_shape[2] * output_shape[3])\n",
    "    x = mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n",
    "    feature_range = tf.range(output_shape[3], dtype=tf.int64)\n",
    "    f = one_like_mask * feature_range\n",
    "\n",
    "    updates_size = tf.size(net)\n",
    "    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n",
    "    values = tf.reshape(net, [updates_size])\n",
    "    ret = tf.scatter_nd(indices, values, output_shape)\n",
    "    return ret\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "data_dir = '../cifar-10-batches-bin'\n",
    "print(\"begin\")\n",
    "images_train, labels_train = cifar10_input.inputs(eval_data = False,data_dir = data_dir, batch_size = batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data = True, data_dir = data_dir, batch_size = batch_size)\n",
    "print(\"begin data\")\n",
    "\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  \n",
    "                        \n",
    "def avg_pool_6x6(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 6, 6, 1], strides=[1, 6, 6, 1], padding='SAME')\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [batch_size, 24,24,3]) # cifar data image of shape 24*24*3\n",
    "y = tf.placeholder(tf.float32, [batch_size, 10]) # 0-9 数字=> 10 classes\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,24,24,3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "#h_pool1 = max_pool_2x2(h_conv1)\n",
    "h_pool1, mask1 = max_pool_with_argmax(h_conv1, 2)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#############################################################\n",
    "h_pool2, mask = max_pool_with_argmax(h_conv2, 2)#(128, 6, 6, 64)\n",
    "print(h_pool2.shape)\n",
    "t_conv2 = unpool(h_pool2, mask, 2)#(128, 12, 12, 64)\n",
    "t_pool1 = tf.nn.conv2d_transpose(t_conv2-b_conv2, W_conv2, h_pool1.shape,[1,1,1,1])#(128, 24, 24, 64)\n",
    "print(t_conv2.shape,h_pool1.shape,t_pool1.shape)\n",
    "t_conv1 = unpool(t_pool1, mask1, 2)\n",
    "t_x_image = tf.nn.conv2d_transpose(t_conv1-b_conv1, W_conv1, x_image.shape,[1,1,1,1])\n",
    "\n",
    "#第一层卷积还原\n",
    "t1_conv1 = unpool(h_pool1, mask1, 2)\n",
    "t1_x_image = tf.nn.conv2d_transpose(t1_conv1-b_conv1, W_conv1, x_image.shape,[1,1,1,1])\n",
    "\n",
    "# 生成最终图像\n",
    "stitched_decodings = tf.concat((x_image, t1_x_image,t_x_image), axis=2)\n",
    "decoding_summary_op = tf.summary.image('source/cifar', stitched_decodings)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 64, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3=avg_pool_6x6(h_conv3)#10\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv=tf.nn.softmax(nt_hpool3_flat)\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv)) +(tf.nn.l2_loss(W_conv1)+tf.nn.l2_loss(W_conv2)+tf.nn.l2_loss(W_conv3))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter('./log/', sess.graph)\n",
    "\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "for i in range(15000):#20000\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    label_b = np.eye(10,dtype=float)[label_batch] #one hot\n",
    "  \n",
    "    train_step.run(feed_dict={x:image_batch, y: label_b},session=sess)\n",
    "  #_, decoding_summary = sess.run([train_step, decoding_summary_op],feed_dict={x:image_batch, y: label_b})\n",
    "  \n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:image_batch, y: label_b},session=sess)\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        print(\"cross_entropy\",cross_entropy.eval(feed_dict={x:image_batch, y: label_b},session=sess))\n",
    "    #summary_writer.add_summary(decoding_summary)\n",
    "\n",
    "\n",
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "label_b = np.eye(10,dtype=float)[label_batch]#one hot\n",
    "print (\"finished！ test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "     x:image_batch, y: label_b},session=sess))\n",
    "decoding_summary = sess.run(decoding_summary_op,feed_dict={x:image_batch, y: label_b})\n",
    "summary_writer.add_summary(decoding_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例51: CIFAR封装代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "begin data\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "step 0, training accuracy 0.148438\n",
      "step 200, training accuracy 0.328125\n",
      "step 400, training accuracy 0.421875\n",
      "step 600, training accuracy 0.34375\n",
      "step 800, training accuracy 0.382812\n",
      "step 1000, training accuracy 0.414062\n",
      "step 1200, training accuracy 0.5\n",
      "step 1400, training accuracy 0.414062\n",
      "step 1600, training accuracy 0.476562\n",
      "step 1800, training accuracy 0.398438\n",
      "step 2000, training accuracy 0.554688\n",
      "step 2200, training accuracy 0.40625\n",
      "step 2400, training accuracy 0.5625\n",
      "step 2600, training accuracy 0.523438\n",
      "step 2800, training accuracy 0.46875\n",
      "step 3000, training accuracy 0.53125\n",
      "step 3200, training accuracy 0.40625\n",
      "step 3400, training accuracy 0.523438\n",
      "step 3600, training accuracy 0.53125\n",
      "step 3800, training accuracy 0.515625\n",
      "step 4000, training accuracy 0.492188\n",
      "step 4200, training accuracy 0.515625\n",
      "step 4400, training accuracy 0.5\n",
      "step 4600, training accuracy 0.617188\n",
      "step 4800, training accuracy 0.546875\n",
      "step 5000, training accuracy 0.539062\n",
      "step 5200, training accuracy 0.578125\n",
      "step 5400, training accuracy 0.5625\n",
      "step 5600, training accuracy 0.492188\n",
      "step 5800, training accuracy 0.554688\n",
      "step 6000, training accuracy 0.632812\n",
      "step 6200, training accuracy 0.523438\n",
      "step 6400, training accuracy 0.523438\n",
      "step 6600, training accuracy 0.523438\n",
      "step 6800, training accuracy 0.492188\n",
      "step 7000, training accuracy 0.570312\n",
      "step 7200, training accuracy 0.523438\n",
      "step 7400, training accuracy 0.601562\n",
      "step 7600, training accuracy 0.601562\n",
      "step 7800, training accuracy 0.570312\n",
      "step 8000, training accuracy 0.625\n",
      "step 8200, training accuracy 0.5\n",
      "step 8400, training accuracy 0.609375\n",
      "step 8600, training accuracy 0.601562\n",
      "step 8800, training accuracy 0.539062\n",
      "step 9000, training accuracy 0.578125\n",
      "step 9200, training accuracy 0.609375\n",
      "step 9400, training accuracy 0.546875\n",
      "step 9600, training accuracy 0.609375\n",
      "step 9800, training accuracy 0.601562\n",
      "step 10000, training accuracy 0.601562\n",
      "step 10200, training accuracy 0.5625\n",
      "step 10400, training accuracy 0.6875\n",
      "step 10600, training accuracy 0.609375\n",
      "step 10800, training accuracy 0.570312\n",
      "step 11000, training accuracy 0.578125\n",
      "step 11200, training accuracy 0.53125\n",
      "step 11400, training accuracy 0.65625\n",
      "step 11600, training accuracy 0.609375\n",
      "step 11800, training accuracy 0.5625\n",
      "step 12000, training accuracy 0.679688\n",
      "step 12200, training accuracy 0.632812\n",
      "step 12400, training accuracy 0.625\n",
      "step 12600, training accuracy 0.65625\n",
      "step 12800, training accuracy 0.632812\n",
      "step 13000, training accuracy 0.585938\n",
      "step 13200, training accuracy 0.539062\n",
      "step 13400, training accuracy 0.640625\n",
      "step 13600, training accuracy 0.617188\n",
      "step 13800, training accuracy 0.679688\n",
      "step 14000, training accuracy 0.664062\n",
      "step 14200, training accuracy 0.609375\n",
      "step 14400, training accuracy 0.585938\n",
      "step 14600, training accuracy 0.65625\n",
      "step 14800, training accuracy 0.609375\n",
      "finished！ test accuracy 0.617188\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "data_dir = '../cifar-10-batches-bin'\n",
    "print(\"begin\")\n",
    "images_train, labels_train = cifar10_input.inputs(eval_data = False,data_dir = data_dir, batch_size = batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data = True, data_dir = data_dir, batch_size = batch_size)\n",
    "print(\"begin data\")\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 24,24,3]) # cifar data image of shape 24*24*3\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 数字=> 10 classes\n",
    "\n",
    "x_image = tf.reshape(x, [-1,24,24,3])\n",
    "\n",
    "h_conv1 =tf.contrib.layers.conv2d(x_image,64,[5,5],1,'SAME',activation_fn=tf.nn.relu)\n",
    "h_pool1 = tf.contrib.layers.max_pool2d(h_conv1,[2,2],stride=2,padding='SAME')\n",
    "\n",
    "h_conv2 =tf.contrib.layers.conv2d(h_pool1,64,[5,5],1,'SAME',activation_fn=tf.nn.relu)\n",
    "h_pool2 = tf.contrib.layers.max_pool2d(h_conv2,[2,2],stride=2,padding='SAME')\n",
    "\n",
    "nt_hpool2 = tf.contrib.layers.avg_pool2d(h_pool2,[6,6],stride=6,padding='SAME')\n",
    "\n",
    "nt_hpool2_flat = tf.reshape(nt_hpool2, [-1, 64])\n",
    "\n",
    "y_conv = tf.contrib.layers.fully_connected(nt_hpool2_flat,10,activation_fn=tf.nn.softmax)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "for i in range(15000):#20000\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    label_b = np.eye(10,dtype=float)[label_batch] #one hot\n",
    "  \n",
    "    train_step.run(feed_dict={x:image_batch, y: label_b},session=sess)\n",
    "  \n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:image_batch, y: label_b},session=sess)\n",
    "        print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "\n",
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "label_b = np.eye(10,dtype=float)[label_batch]#one hot\n",
    "print (\"finished！ test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "     x:image_batch, y: label_b},session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例52：cifar卷积核优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:209: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:65: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1241: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:113: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "begin data\n",
      "WARNING:tensorflow:From <ipython-input-1-d3f207e25126>:77: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "step 0, training accuracy 0.125\n",
      "step 200, training accuracy 0.34375\n",
      "step 400, training accuracy 0.375\n",
      "step 600, training accuracy 0.367188\n",
      "step 800, training accuracy 0.507812\n",
      "step 1000, training accuracy 0.382812\n",
      "step 1200, training accuracy 0.429688\n",
      "step 1400, training accuracy 0.5\n",
      "step 1600, training accuracy 0.4375\n",
      "step 1800, training accuracy 0.390625\n",
      "step 2000, training accuracy 0.515625\n",
      "step 2200, training accuracy 0.59375\n",
      "step 2400, training accuracy 0.554688\n",
      "step 2600, training accuracy 0.539062\n",
      "step 2800, training accuracy 0.53125\n",
      "step 3000, training accuracy 0.5\n",
      "step 3200, training accuracy 0.414062\n",
      "step 3400, training accuracy 0.515625\n",
      "step 3600, training accuracy 0.515625\n",
      "step 3800, training accuracy 0.476562\n",
      "step 4000, training accuracy 0.585938\n",
      "step 4200, training accuracy 0.5625\n",
      "step 4400, training accuracy 0.570312\n",
      "step 4600, training accuracy 0.523438\n",
      "step 4800, training accuracy 0.523438\n",
      "step 5000, training accuracy 0.5625\n",
      "step 5200, training accuracy 0.53125\n",
      "step 5400, training accuracy 0.59375\n",
      "step 5600, training accuracy 0.53125\n",
      "step 5800, training accuracy 0.601562\n",
      "step 6000, training accuracy 0.585938\n",
      "step 6200, training accuracy 0.617188\n",
      "step 6400, training accuracy 0.59375\n",
      "step 6600, training accuracy 0.617188\n",
      "step 6800, training accuracy 0.570312\n",
      "step 7000, training accuracy 0.585938\n",
      "step 7200, training accuracy 0.664062\n",
      "step 7400, training accuracy 0.648438\n",
      "step 7600, training accuracy 0.65625\n",
      "step 7800, training accuracy 0.632812\n",
      "step 8000, training accuracy 0.65625\n",
      "step 8200, training accuracy 0.585938\n",
      "step 8400, training accuracy 0.578125\n",
      "step 8600, training accuracy 0.585938\n",
      "step 8800, training accuracy 0.578125\n",
      "step 9000, training accuracy 0.695312\n",
      "step 9200, training accuracy 0.570312\n",
      "step 9400, training accuracy 0.546875\n",
      "step 9600, training accuracy 0.742188\n",
      "step 9800, training accuracy 0.6875\n",
      "step 10000, training accuracy 0.59375\n",
      "step 10200, training accuracy 0.5625\n",
      "step 10400, training accuracy 0.679688\n",
      "step 10600, training accuracy 0.617188\n",
      "step 10800, training accuracy 0.65625\n",
      "step 11000, training accuracy 0.578125\n",
      "step 11200, training accuracy 0.679688\n",
      "step 11400, training accuracy 0.617188\n",
      "step 11600, training accuracy 0.609375\n",
      "step 11800, training accuracy 0.632812\n",
      "step 12000, training accuracy 0.671875\n",
      "step 12200, training accuracy 0.664062\n",
      "step 12400, training accuracy 0.609375\n",
      "step 12600, training accuracy 0.710938\n",
      "step 12800, training accuracy 0.617188\n",
      "step 13000, training accuracy 0.679688\n",
      "step 13200, training accuracy 0.625\n",
      "step 13400, training accuracy 0.632812\n",
      "step 13600, training accuracy 0.554688\n",
      "step 13800, training accuracy 0.65625\n",
      "step 14000, training accuracy 0.679688\n",
      "step 14200, training accuracy 0.710938\n",
      "step 14400, training accuracy 0.554688\n",
      "step 14600, training accuracy 0.75\n",
      "step 14800, training accuracy 0.632812\n",
      "finished！ test accuracy 0.609375\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "data_dir = '../cifar-10-batches-bin'\n",
    "print(\"begin\")\n",
    "images_train, labels_train = cifar10_input.inputs(eval_data = False,data_dir = data_dir, batch_size = batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data = True, data_dir = data_dir, batch_size = batch_size)\n",
    "print(\"begin data\")\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')  \n",
    "                        \n",
    "def avg_pool_6x6(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 6, 6, 1],\n",
    "                        strides=[1, 6, 6, 1], padding='SAME')\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 24,24,3]) # cifar data image of shape 24*24*3\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 数字=> 10 classes\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,24,24,3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "#########################################################new########\n",
    "W_conv21 = weight_variable([5, 1, 64, 64])\n",
    "b_conv21 = bias_variable([64])\n",
    "h_conv21 = tf.nn.relu(conv2d(h_pool1, W_conv21) + b_conv21)\n",
    "\n",
    "W_conv2 = weight_variable([1, 5, 64, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv21, W_conv2) + b_conv2)\n",
    "###########################################################old#########\n",
    "#W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "#b_conv2 = bias_variable([64])\n",
    "#h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "###################################################################\n",
    "\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "W_conv3 = weight_variable([5, 5, 64, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3=avg_pool_6x6(h_conv3)#10\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv=tf.nn.softmax(nt_hpool3_flat)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "for i in range(15000):#20000\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    label_b = np.eye(10,dtype=float)[label_batch] #one hot\n",
    "  \n",
    "    train_step.run(feed_dict={x:image_batch, y: label_b},session=sess)\n",
    "  \n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:image_batch, y: label_b},session=sess)\n",
    "        print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "\n",
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "label_b = np.eye(10,dtype=float)[label_batch]#one hot\n",
    "print (\"finished！ test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "     x:image_batch, y: label_b},session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例53:cifar多通道卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "begin data\n",
      "step 0, training accuracy 0.09375\n",
      "step 200, training accuracy 0.304688\n",
      "step 400, training accuracy 0.398438\n",
      "step 600, training accuracy 0.4375\n",
      "step 800, training accuracy 0.507812\n",
      "step 1000, training accuracy 0.46875\n",
      "step 1200, training accuracy 0.421875\n",
      "step 1400, training accuracy 0.585938\n",
      "step 1600, training accuracy 0.585938\n",
      "step 1800, training accuracy 0.484375\n",
      "step 2000, training accuracy 0.546875\n",
      "step 2200, training accuracy 0.539062\n",
      "step 2400, training accuracy 0.625\n",
      "step 2600, training accuracy 0.570312\n",
      "step 2800, training accuracy 0.570312\n",
      "step 3000, training accuracy 0.546875\n",
      "step 3200, training accuracy 0.601562\n",
      "step 3400, training accuracy 0.609375\n",
      "step 3600, training accuracy 0.609375\n",
      "step 3800, training accuracy 0.625\n",
      "step 4000, training accuracy 0.671875\n",
      "step 4200, training accuracy 0.625\n",
      "step 4400, training accuracy 0.601562\n",
      "step 4600, training accuracy 0.609375\n",
      "step 4800, training accuracy 0.570312\n",
      "step 5000, training accuracy 0.65625\n",
      "step 5200, training accuracy 0.640625\n",
      "step 5400, training accuracy 0.671875\n",
      "step 5600, training accuracy 0.5625\n",
      "step 5800, training accuracy 0.65625\n",
      "step 6000, training accuracy 0.695312\n",
      "step 6200, training accuracy 0.757812\n",
      "step 6400, training accuracy 0.648438\n",
      "step 6600, training accuracy 0.734375\n",
      "step 6800, training accuracy 0.632812\n",
      "step 7000, training accuracy 0.6875\n",
      "step 7200, training accuracy 0.679688\n",
      "step 7400, training accuracy 0.695312\n",
      "step 7600, training accuracy 0.75\n",
      "step 7800, training accuracy 0.65625\n",
      "step 8000, training accuracy 0.71875\n",
      "step 8200, training accuracy 0.617188\n",
      "step 8400, training accuracy 0.632812\n",
      "step 8600, training accuracy 0.679688\n",
      "step 8800, training accuracy 0.679688\n",
      "step 9000, training accuracy 0.625\n",
      "step 9200, training accuracy 0.710938\n",
      "step 9400, training accuracy 0.625\n",
      "step 9600, training accuracy 0.765625\n",
      "step 9800, training accuracy 0.679688\n",
      "step 10000, training accuracy 0.710938\n",
      "step 10200, training accuracy 0.710938\n",
      "step 10400, training accuracy 0.71875\n",
      "step 10600, training accuracy 0.679688\n",
      "step 10800, training accuracy 0.671875\n",
      "step 11000, training accuracy 0.75\n",
      "step 11200, training accuracy 0.609375\n",
      "step 11400, training accuracy 0.6875\n",
      "step 11600, training accuracy 0.695312\n",
      "step 11800, training accuracy 0.6875\n",
      "step 12000, training accuracy 0.75\n",
      "step 12200, training accuracy 0.773438\n",
      "step 12400, training accuracy 0.71875\n",
      "step 12600, training accuracy 0.671875\n",
      "step 12800, training accuracy 0.679688\n",
      "step 13000, training accuracy 0.664062\n",
      "step 13200, training accuracy 0.710938\n",
      "step 13400, training accuracy 0.773438\n",
      "step 13600, training accuracy 0.734375\n",
      "step 13800, training accuracy 0.6875\n",
      "step 14000, training accuracy 0.75\n",
      "step 14200, training accuracy 0.765625\n",
      "step 14400, training accuracy 0.757812\n",
      "step 14600, training accuracy 0.796875\n",
      "step 14800, training accuracy 0.773438\n",
      "finished！ test accuracy 0.625\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "data_dir = '../cifar-10-batches-bin'\n",
    "print(\"begin\")\n",
    "images_train, labels_train = cifar10_input.inputs(eval_data = False,data_dir = data_dir, batch_size = batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data = True, data_dir = data_dir, batch_size = batch_size)\n",
    "print(\"begin data\")\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')  \n",
    "                        \n",
    "def avg_pool_6x6(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 6, 6, 1],\n",
    "                        strides=[1, 6, 6, 1], padding='SAME')\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 24,24,3]) # cifar data image of shape 24*24*3\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 数字=> 10 classes\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,24,24,3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "#######################################################多卷积核\n",
    "W_conv2_5x5 = weight_variable([5, 5, 64, 64]) \n",
    "b_conv2_5x5 = bias_variable([64]) \n",
    "W_conv2_7x7 = weight_variable([7, 7, 64, 64]) \n",
    "b_conv2_7x7 = bias_variable([64]) \n",
    "\n",
    "W_conv2_3x3 = weight_variable([3, 3, 64, 64]) \n",
    "b_conv2_3x3 = bias_variable([64]) \n",
    "\n",
    "W_conv2_1x1 = weight_variable([1, 1, 64, 64]) \n",
    "b_conv2_1x1 = bias_variable([64]) \n",
    "\n",
    "h_conv2_1x1 = tf.nn.relu(conv2d(h_pool1, W_conv2_1x1) + b_conv2_1x1)\n",
    "h_conv2_3x3 = tf.nn.relu(conv2d(h_pool1, W_conv2_3x3) + b_conv2_3x3)\n",
    "h_conv2_5x5 = tf.nn.relu(conv2d(h_pool1, W_conv2_5x5) + b_conv2_5x5)\n",
    "h_conv2_7x7 = tf.nn.relu(conv2d(h_pool1, W_conv2_7x7) + b_conv2_7x7)\n",
    "h_conv2 = tf.concat([h_conv2_5x5,h_conv2_7x7,h_conv2_3x3,h_conv2_1x1],3)\n",
    "\n",
    "#######################################################\n",
    "#W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "#b_conv2 = bias_variable([64])\n",
    "#\n",
    "#h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "#######################################################\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 256, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3=avg_pool_6x6(h_conv3)#10\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv=tf.nn.softmax(nt_hpool3_flat)\n",
    "\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))\n",
    "\n",
    "#不同的优化方法测测效果\n",
    "#train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(cross_entropy)\n",
    "#train_step = tf.train.AdagradOptimizer(1e-5).minimize(cross_entropy)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "for i in range(15000):#20000\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    label_b = np.eye(10,dtype=float)[label_batch] #one hot\n",
    "  \n",
    "    train_step.run(feed_dict={x:image_batch, y: label_b},session=sess)\n",
    "  \n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:image_batch, y: label_b},session=sess)\n",
    "        print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "\n",
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "label_b = np.eye(10,dtype=float)[label_batch]#one hot\n",
    "print (\"finished！ test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "     x:image_batch, y: label_b},session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例54：BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:209: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:65: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1241: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From H:\\tensorflow_projects\\chap8\\cifar10\\cifar10_input.py:113: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "begin data\n",
      "WARNING:tensorflow:From <ipython-input-1-be8cf252bbfc>:83: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "step 0, training accuracy 0.15625\n",
      "step 200, training accuracy 0.078125\n",
      "step 400, training accuracy 0.117188\n",
      "step 600, training accuracy 0.0859375\n",
      "step 800, training accuracy 0.0625\n",
      "step 1000, training accuracy 0.125\n",
      "step 1200, training accuracy 0.109375\n",
      "step 1400, training accuracy 0.09375\n",
      "step 1600, training accuracy 0.078125\n",
      "step 1800, training accuracy 0.0859375\n",
      "step 2000, training accuracy 0.109375\n",
      "step 2200, training accuracy 0.0859375\n",
      "step 2400, training accuracy 0.117188\n",
      "step 2600, training accuracy 0.09375\n",
      "step 2800, training accuracy 0.0625\n",
      "step 3000, training accuracy 0.09375\n",
      "step 3200, training accuracy 0.109375\n",
      "step 3400, training accuracy 0.0625\n",
      "step 3600, training accuracy 0.109375\n",
      "step 3800, training accuracy 0.078125\n",
      "step 4000, training accuracy 0.125\n",
      "step 4200, training accuracy 0.09375\n",
      "step 4400, training accuracy 0.117188\n",
      "step 4600, training accuracy 0.125\n",
      "step 4800, training accuracy 0.101562\n",
      "step 5000, training accuracy 0.09375\n",
      "step 5200, training accuracy 0.078125\n",
      "step 5400, training accuracy 0.132812\n",
      "step 5600, training accuracy 0.078125\n",
      "step 5800, training accuracy 0.117188\n",
      "step 6000, training accuracy 0.132812\n",
      "step 6200, training accuracy 0.109375\n",
      "step 6400, training accuracy 0.101562\n",
      "step 6600, training accuracy 0.078125\n",
      "step 6800, training accuracy 0.09375\n",
      "step 7000, training accuracy 0.09375\n",
      "step 7200, training accuracy 0.117188\n",
      "step 7400, training accuracy 0.125\n",
      "step 7600, training accuracy 0.109375\n",
      "step 7800, training accuracy 0.101562\n",
      "step 8000, training accuracy 0.109375\n",
      "step 8200, training accuracy 0.109375\n",
      "step 8400, training accuracy 0.078125\n",
      "step 8600, training accuracy 0.078125\n",
      "step 8800, training accuracy 0.09375\n",
      "step 9000, training accuracy 0.0859375\n",
      "step 9200, training accuracy 0.109375\n",
      "step 9400, training accuracy 0.117188\n",
      "step 9600, training accuracy 0.0859375\n",
      "step 9800, training accuracy 0.109375\n",
      "step 10000, training accuracy 0.09375\n",
      "step 10200, training accuracy 0.109375\n",
      "step 10400, training accuracy 0.117188\n",
      "step 10600, training accuracy 0.09375\n",
      "step 10800, training accuracy 0.0625\n",
      "step 11000, training accuracy 0.132812\n",
      "step 11200, training accuracy 0.078125\n",
      "step 11400, training accuracy 0.101562\n",
      "step 11600, training accuracy 0.109375\n",
      "step 11800, training accuracy 0.117188\n",
      "step 12000, training accuracy 0.0859375\n",
      "step 12200, training accuracy 0.0703125\n",
      "step 12400, training accuracy 0.117188\n",
      "step 12600, training accuracy 0.078125\n",
      "step 12800, training accuracy 0.078125\n",
      "step 13000, training accuracy 0.148438\n",
      "step 13200, training accuracy 0.101562\n",
      "step 13400, training accuracy 0.140625\n",
      "step 13600, training accuracy 0.0859375\n",
      "step 13800, training accuracy 0.101562\n",
      "step 14000, training accuracy 0.09375\n",
      "step 14200, training accuracy 0.09375\n",
      "step 14400, training accuracy 0.09375\n",
      "step 14600, training accuracy 0.125\n",
      "step 14800, training accuracy 0.109375\n",
      "step 15000, training accuracy 0.09375\n",
      "step 15200, training accuracy 0.0859375\n",
      "step 15400, training accuracy 0.109375\n",
      "step 15600, training accuracy 0.0859375\n",
      "step 15800, training accuracy 0.078125\n",
      "step 16000, training accuracy 0.125\n",
      "step 16200, training accuracy 0.125\n",
      "step 16400, training accuracy 0.101562\n",
      "step 16600, training accuracy 0.101562\n",
      "step 16800, training accuracy 0.132812\n",
      "step 17000, training accuracy 0.117188\n",
      "step 17200, training accuracy 0.0859375\n",
      "step 17400, training accuracy 0.0859375\n",
      "step 17600, training accuracy 0.0859375\n",
      "step 17800, training accuracy 0.09375\n",
      "step 18000, training accuracy 0.109375\n",
      "step 18200, training accuracy 0.101562\n",
      "step 18400, training accuracy 0.0703125\n",
      "step 18600, training accuracy 0.109375\n",
      "step 18800, training accuracy 0.09375\n",
      "step 19000, training accuracy 0.09375\n",
      "step 19200, training accuracy 0.03125\n",
      "step 19400, training accuracy 0.0546875\n",
      "step 19600, training accuracy 0.109375\n",
      "step 19800, training accuracy 0.0625\n",
      "finished！ test accuracy 0.101562\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm\n",
    "\n",
    "batch_size = 128\n",
    "data_dir = '../cifar-10-batches-bin'\n",
    "print(\"begin\")\n",
    "images_train, labels_train = cifar10_input.inputs(eval_data = False,data_dir = data_dir, batch_size = batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data = True, data_dir = data_dir, batch_size = batch_size)\n",
    "print(\"begin data\")\n",
    "\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')  \n",
    "                        \n",
    "def avg_pool_6x6(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 6, 6, 1],\n",
    "                        strides=[1, 6, 6, 1], padding='SAME')\n",
    "                        \n",
    "def batch_norm_layer(value,train = None, name = 'batch_norm'): \n",
    "    if train is not None:       \n",
    "        return batch_norm(value, decay = 0.9,updates_collections=None, is_training = True)\n",
    "    else:\n",
    "        return batch_norm(value, decay = 0.9,updates_collections=None, is_training = False)\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 24,24,3]) # cifar data image of shape 24*24*3\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 数字=> 10 classes\n",
    "train = tf.placeholder(tf.float32)\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,24,24,3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(batch_norm_layer((conv2d(x_image, W_conv1) + b_conv1),train))\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(batch_norm_layer((conv2d(h_pool1, W_conv2) + b_conv2),train))\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 64, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3=avg_pool_6x6(h_conv3)#10\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv=tf.nn.softmax(nt_hpool3_flat)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "decaylearning_rate = tf.train.exponential_decay(0.04, global_step,1000, 0.9)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(decaylearning_rate).minimize(cross_entropy,global_step=global_step)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "for i in range(20000):\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    label_b = np.eye(10,dtype=float)[label_batch] #one hot\n",
    "  \n",
    "    train_step.run(feed_dict={x:image_batch, y: label_b,train:1},session=sess)\n",
    "  \n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:image_batch, y: label_b},session=sess)\n",
    "        print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "\n",
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "label_b = np.eye(10,dtype=float)[label_batch]#one hot\n",
    "print (\"finished！ test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "     x:image_batch, y: label_b},session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带BN的多通道mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-cea6c263abdb>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "step 0, training accuracy 0.04\n",
      "step 100, training accuracy 0.78\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 0.94\n",
      "step 400, training accuracy 0.98\n",
      "step 500, training accuracy 0.98\n",
      "step 600, training accuracy 0.98\n",
      "step 700, training accuracy 1\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 0.94\n",
      "step 1000, training accuracy 0.96\n",
      "step 1100, training accuracy 0.98\n",
      "step 1200, training accuracy 0.94\n",
      "step 1300, training accuracy 0.94\n",
      "step 1400, training accuracy 1\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.98\n",
      "step 1800, training accuracy 0.98\n",
      "step 1900, training accuracy 0.98\n",
      "step 2000, training accuracy 0.98\n",
      "step 2100, training accuracy 1\n",
      "step 2200, training accuracy 0.96\n",
      "step 2300, training accuracy 0.96\n",
      "step 2400, training accuracy 1\n",
      "step 2500, training accuracy 0.96\n",
      "step 2600, training accuracy 0.98\n",
      "step 2700, training accuracy 0.98\n",
      "step 2800, training accuracy 1\n",
      "step 2900, training accuracy 1\n",
      "step 3000, training accuracy 0.98\n",
      "step 3100, training accuracy 1\n",
      "step 3200, training accuracy 0.98\n",
      "step 3300, training accuracy 0.98\n",
      "step 3400, training accuracy 0.94\n",
      "step 3500, training accuracy 1\n",
      "step 3600, training accuracy 0.98\n",
      "step 3700, training accuracy 0.96\n",
      "step 3800, training accuracy 1\n",
      "step 3900, training accuracy 1\n",
      "step 4000, training accuracy 0.98\n",
      "step 4100, training accuracy 0.98\n",
      "step 4200, training accuracy 1\n",
      "step 4300, training accuracy 1\n",
      "step 4400, training accuracy 0.98\n",
      "step 4500, training accuracy 0.98\n",
      "step 4600, training accuracy 1\n",
      "step 4700, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 4900, training accuracy 1\n",
      "step 5000, training accuracy 0.98\n",
      "step 5100, training accuracy 0.98\n",
      "step 5200, training accuracy 0.98\n",
      "step 5300, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5500, training accuracy 0.96\n",
      "step 5600, training accuracy 1\n",
      "step 5700, training accuracy 0.98\n",
      "step 5800, training accuracy 0.96\n",
      "step 5900, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6100, training accuracy 0.98\n",
      "step 6200, training accuracy 0.98\n",
      "step 6300, training accuracy 1\n",
      "step 6400, training accuracy 0.96\n",
      "step 6500, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6700, training accuracy 0.98\n",
      "step 6800, training accuracy 1\n",
      "step 6900, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7100, training accuracy 1\n",
      "step 7200, training accuracy 0.94\n",
      "step 7300, training accuracy 1\n",
      "step 7400, training accuracy 0.96\n",
      "step 7500, training accuracy 0.98\n",
      "step 7600, training accuracy 1\n",
      "step 7700, training accuracy 1\n",
      "step 7800, training accuracy 0.98\n",
      "step 7900, training accuracy 0.98\n",
      "step 8000, training accuracy 1\n",
      "step 8100, training accuracy 0.98\n",
      "step 8200, training accuracy 1\n",
      "step 8300, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8500, training accuracy 0.98\n",
      "step 8600, training accuracy 1\n",
      "step 8700, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 8900, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9100, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9300, training accuracy 0.98\n",
      "step 9400, training accuracy 1\n",
      "step 9500, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9700, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 9900, training accuracy 0.96\n",
      "step 10000, training accuracy 1\n",
      "step 10100, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10300, training accuracy 1\n",
      "step 10400, training accuracy 0.98\n",
      "step 10500, training accuracy 0.96\n",
      "step 10600, training accuracy 0.98\n",
      "step 10700, training accuracy 0.98\n",
      "step 10800, training accuracy 0.98\n",
      "step 10900, training accuracy 1\n",
      "step 11000, training accuracy 0.98\n",
      "step 11100, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11300, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11500, training accuracy 1\n",
      "step 11600, training accuracy 0.98\n",
      "step 11700, training accuracy 1\n",
      "step 11800, training accuracy 0.98\n",
      "step 11900, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12100, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12300, training accuracy 0.98\n",
      "step 12400, training accuracy 1\n",
      "step 12500, training accuracy 1\n",
      "step 12600, training accuracy 0.98\n",
      "step 12700, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 12900, training accuracy 1\n",
      "step 13000, training accuracy 0.98\n",
      "step 13100, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13300, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13500, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13700, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 13900, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14100, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14300, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14500, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14700, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 14900, training accuracy 0.98\n",
      "step 15000, training accuracy 1\n",
      "step 15100, training accuracy 0.98\n",
      "step 15200, training accuracy 1\n",
      "step 15300, training accuracy 1\n",
      "step 15400, training accuracy 0.98\n",
      "step 15500, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15700, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 15900, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16100, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16300, training accuracy 0.98\n",
      "step 16400, training accuracy 1\n",
      "step 16500, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16700, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 16900, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17100, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17300, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17500, training accuracy 1\n",
      "step 17600, training accuracy 0.96\n",
      "step 17700, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 17900, training accuracy 1\n",
      "step 18000, training accuracy 0.98\n",
      "step 18100, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18300, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18500, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18600, training accuracy 1\n",
      "step 18700, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 18900, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19100, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19300, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19500, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19700, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "step 19900, training accuracy 1\n",
      "test accuracy 0.9933\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm\n",
    "# 导入 MINST 数据集\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/data/\", one_hot=True)\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')  \n",
    "def avg_pool_7x7(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 7, 7, 1],\n",
    "                        strides=[1, 7, 7, 1], padding='SAME')\n",
    "def batch_norm_layer(value,train = None, name = 'batch_norm'): \n",
    "    if train is not None:       \n",
    "        return batch_norm(value, decay = 0.9,updates_collections=None, is_training = True)\n",
    "    else:\n",
    "        return batch_norm(value, decay = 0.9,updates_collections=None, is_training = False)\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data维度 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 数字=> 10 classes\n",
    "train = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(batch_norm_layer((conv2d(x_image, W_conv1) + b_conv1),train))\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "######################################################多卷积核\n",
    "W_conv2_5x5 = weight_variable([5, 5, 32, 32]) \n",
    "b_conv2_5x5 = bias_variable([32]) \n",
    "W_conv2_7x7 = weight_variable([7, 7, 32, 32]) \n",
    "b_conv2_7x7 = bias_variable([32]) \n",
    "h_conv2_5x5 = tf.nn.relu(batch_norm_layer((conv2d(h_pool1, W_conv2_5x5) + b_conv2_5x5),train))\n",
    "h_conv2_7x7 = tf.nn.relu(batch_norm_layer((conv2d(h_pool1, W_conv2_7x7) + b_conv2_7x7),train))\n",
    "h_conv2 = tf.concat([h_conv2_5x5,h_conv2_7x7],3)\n",
    "\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "#########################################################new 池化\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 64, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3=avg_pool_7x7(h_conv3)#10\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv=tf.nn.softmax(nt_hpool3_flat)\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))\n",
    "\n",
    "decaylearning_rate = tf.train.exponential_decay(0.04, 20000,1000, 0.9)\n",
    "train_step = tf.train.AdamOptimizer(decaylearning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# 启动session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):#20000\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:batch[0], y: batch[1], keep_prob: 1.0})\n",
    "            print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    print (\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多通道mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/train-images-idx3-ubyte.gz\n",
      "Extracting /data/train-labels-idx1-ubyte.gz\n",
      "Extracting /data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.14\n",
      "step 100, training accuracy 0.08\n",
      "step 200, training accuracy 0.44\n",
      "step 300, training accuracy 0.52\n",
      "step 400, training accuracy 0.44\n",
      "step 500, training accuracy 0.36\n",
      "step 600, training accuracy 0.68\n",
      "step 700, training accuracy 0.5\n",
      "step 800, training accuracy 0.74\n",
      "step 900, training accuracy 0.7\n",
      "step 1000, training accuracy 0.76\n",
      "step 1100, training accuracy 0.8\n",
      "step 1200, training accuracy 0.84\n",
      "step 1300, training accuracy 0.8\n",
      "step 1400, training accuracy 0.84\n",
      "step 1500, training accuracy 0.82\n",
      "step 1600, training accuracy 0.86\n",
      "step 1700, training accuracy 0.88\n",
      "step 1800, training accuracy 0.88\n",
      "step 1900, training accuracy 0.9\n",
      "step 2000, training accuracy 0.98\n",
      "step 2100, training accuracy 0.9\n",
      "step 2200, training accuracy 0.92\n",
      "step 2300, training accuracy 0.94\n",
      "step 2400, training accuracy 0.9\n",
      "step 2500, training accuracy 0.98\n",
      "step 2600, training accuracy 0.92\n",
      "step 2700, training accuracy 0.92\n",
      "step 2800, training accuracy 0.88\n",
      "step 2900, training accuracy 0.92\n",
      "step 3000, training accuracy 0.9\n",
      "step 3100, training accuracy 0.92\n",
      "step 3200, training accuracy 0.94\n",
      "step 3300, training accuracy 0.92\n",
      "step 3400, training accuracy 0.92\n",
      "step 3500, training accuracy 0.96\n",
      "step 3600, training accuracy 0.92\n",
      "step 3700, training accuracy 0.88\n",
      "step 3800, training accuracy 0.96\n",
      "step 3900, training accuracy 0.94\n",
      "step 4000, training accuracy 0.98\n",
      "step 4100, training accuracy 0.86\n",
      "step 4200, training accuracy 0.94\n",
      "step 4300, training accuracy 0.88\n",
      "step 4400, training accuracy 0.92\n",
      "step 4500, training accuracy 0.94\n",
      "step 4600, training accuracy 0.94\n",
      "step 4700, training accuracy 0.88\n",
      "step 4800, training accuracy 0.94\n",
      "step 4900, training accuracy 0.94\n",
      "step 5000, training accuracy 0.88\n",
      "step 5100, training accuracy 0.92\n",
      "step 5200, training accuracy 0.98\n",
      "step 5300, training accuracy 0.88\n",
      "step 5400, training accuracy 0.94\n",
      "step 5500, training accuracy 0.92\n",
      "step 5600, training accuracy 0.94\n",
      "step 5700, training accuracy 0.96\n",
      "step 5800, training accuracy 0.96\n",
      "step 5900, training accuracy 0.98\n",
      "step 6000, training accuracy 1\n",
      "step 6100, training accuracy 0.94\n",
      "step 6200, training accuracy 0.98\n",
      "step 6300, training accuracy 0.96\n",
      "step 6400, training accuracy 0.92\n",
      "step 6500, training accuracy 0.94\n",
      "step 6600, training accuracy 0.98\n",
      "step 6700, training accuracy 0.92\n",
      "step 6800, training accuracy 0.94\n",
      "step 6900, training accuracy 0.96\n",
      "step 7000, training accuracy 1\n",
      "step 7100, training accuracy 0.94\n",
      "step 7200, training accuracy 0.92\n",
      "step 7300, training accuracy 0.96\n",
      "step 7400, training accuracy 0.98\n",
      "step 7500, training accuracy 0.96\n",
      "step 7600, training accuracy 0.94\n",
      "step 7700, training accuracy 0.96\n",
      "step 7800, training accuracy 0.96\n",
      "step 7900, training accuracy 0.98\n",
      "step 8000, training accuracy 0.96\n",
      "step 8100, training accuracy 0.92\n",
      "step 8200, training accuracy 0.98\n",
      "step 8300, training accuracy 0.98\n",
      "step 8400, training accuracy 0.96\n",
      "step 8500, training accuracy 0.96\n",
      "step 8600, training accuracy 0.96\n",
      "step 8700, training accuracy 0.94\n",
      "step 8800, training accuracy 0.98\n",
      "step 8900, training accuracy 0.96\n",
      "step 9000, training accuracy 0.98\n",
      "step 9100, training accuracy 0.9\n",
      "step 9200, training accuracy 0.96\n",
      "step 9300, training accuracy 0.98\n",
      "step 9400, training accuracy 0.88\n",
      "step 9500, training accuracy 0.98\n",
      "step 9600, training accuracy 0.98\n",
      "step 9700, training accuracy 1\n",
      "step 9800, training accuracy 0.92\n",
      "step 9900, training accuracy 0.96\n",
      "step 10000, training accuracy 0.98\n",
      "step 10100, training accuracy 0.96\n",
      "step 10200, training accuracy 0.98\n",
      "step 10300, training accuracy 0.9\n",
      "step 10400, training accuracy 0.92\n",
      "step 10500, training accuracy 0.96\n",
      "step 10600, training accuracy 0.96\n",
      "step 10700, training accuracy 0.94\n",
      "step 10800, training accuracy 0.98\n",
      "step 10900, training accuracy 0.98\n",
      "step 11000, training accuracy 0.98\n",
      "step 11100, training accuracy 0.98\n",
      "step 11200, training accuracy 1\n",
      "step 11300, training accuracy 0.92\n",
      "step 11400, training accuracy 1\n",
      "step 11500, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11700, training accuracy 0.94\n",
      "step 11800, training accuracy 0.98\n",
      "step 11900, training accuracy 0.98\n",
      "step 12000, training accuracy 0.96\n",
      "step 12100, training accuracy 0.94\n",
      "step 12200, training accuracy 0.96\n",
      "step 12300, training accuracy 0.98\n",
      "step 12400, training accuracy 0.94\n",
      "step 12500, training accuracy 0.92\n",
      "step 12600, training accuracy 0.92\n",
      "step 12700, training accuracy 0.98\n",
      "step 12800, training accuracy 0.98\n",
      "step 12900, training accuracy 0.98\n",
      "step 13000, training accuracy 0.96\n",
      "step 13100, training accuracy 0.96\n",
      "step 13200, training accuracy 0.98\n",
      "step 13300, training accuracy 0.96\n",
      "step 13400, training accuracy 0.92\n",
      "step 13500, training accuracy 0.96\n",
      "step 13600, training accuracy 0.98\n",
      "step 13700, training accuracy 0.94\n",
      "step 13800, training accuracy 0.98\n",
      "step 13900, training accuracy 0.98\n",
      "step 14000, training accuracy 0.98\n",
      "step 14100, training accuracy 0.9\n",
      "step 14200, training accuracy 0.98\n",
      "step 14300, training accuracy 1\n",
      "step 14400, training accuracy 0.9\n",
      "step 14500, training accuracy 0.98\n",
      "step 14600, training accuracy 0.92\n",
      "step 14700, training accuracy 0.94\n",
      "step 14800, training accuracy 0.94\n",
      "step 14900, training accuracy 0.98\n",
      "step 15000, training accuracy 1\n",
      "step 15100, training accuracy 1\n",
      "step 15200, training accuracy 0.98\n",
      "step 15300, training accuracy 0.94\n",
      "step 15400, training accuracy 0.96\n",
      "step 15500, training accuracy 0.94\n",
      "step 15600, training accuracy 0.98\n",
      "step 15700, training accuracy 0.98\n",
      "step 15800, training accuracy 0.92\n",
      "step 15900, training accuracy 0.94\n",
      "step 16000, training accuracy 0.98\n",
      "step 16100, training accuracy 0.98\n",
      "step 16200, training accuracy 0.98\n",
      "step 16300, training accuracy 1\n",
      "step 16400, training accuracy 0.96\n",
      "step 16500, training accuracy 0.94\n",
      "step 16600, training accuracy 0.96\n",
      "step 16700, training accuracy 0.96\n",
      "step 16800, training accuracy 0.98\n",
      "step 16900, training accuracy 0.96\n",
      "step 17000, training accuracy 1\n",
      "step 17100, training accuracy 0.98\n",
      "step 17200, training accuracy 0.98\n",
      "step 17300, training accuracy 0.94\n",
      "step 17400, training accuracy 1\n",
      "step 17500, training accuracy 0.94\n",
      "step 17600, training accuracy 0.96\n",
      "step 17700, training accuracy 1\n",
      "step 17800, training accuracy 0.96\n",
      "step 17900, training accuracy 0.92\n",
      "step 18000, training accuracy 0.98\n",
      "step 18100, training accuracy 0.96\n",
      "step 18200, training accuracy 0.96\n",
      "step 18300, training accuracy 0.98\n",
      "step 18400, training accuracy 0.98\n",
      "step 18500, training accuracy 0.98\n",
      "step 18600, training accuracy 0.98\n",
      "step 18700, training accuracy 0.98\n",
      "step 18800, training accuracy 0.98\n",
      "step 18900, training accuracy 0.98\n",
      "step 19000, training accuracy 1\n",
      "step 19100, training accuracy 0.98\n",
      "step 19200, training accuracy 0.98\n",
      "step 19300, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19500, training accuracy 0.98\n",
      "step 19600, training accuracy 0.96\n",
      "step 19700, training accuracy 0.98\n",
      "step 19800, training accuracy 0.98\n",
      "step 19900, training accuracy 1\n",
      "test accuracy 0.9768\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# 导入 MINST 数据集\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/data/\", one_hot=True)\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')  \n",
    "def avg_pool_7x7(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 7, 7, 1],\n",
    "                        strides=[1, 7, 7, 1], padding='SAME')\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data维度 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 数字=> 10 classes\n",
    "\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "######################################################多卷积核\n",
    "W_conv2_5x5 = weight_variable([5, 5, 32, 32]) \n",
    "b_conv2_5x5 = bias_variable([32]) \n",
    "W_conv2_7x7 = weight_variable([7, 7, 32, 32]) \n",
    "b_conv2_7x7 = bias_variable([32]) \n",
    "h_conv2_5x5 = tf.nn.relu(conv2d(h_pool1, W_conv2_5x5) + b_conv2_5x5)\n",
    "h_conv2_7x7 = tf.nn.relu(conv2d(h_pool1, W_conv2_7x7) + b_conv2_7x7)\n",
    "h_conv2 = tf.concat([h_conv2_5x5,h_conv2_7x7],3)\n",
    "\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "#########################################################new 池化\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 64, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3=avg_pool_7x7(h_conv3)#10\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv=tf.nn.softmax(nt_hpool3_flat)\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv))\n",
    "\n",
    "#不同的优化方法测测效果\n",
    "#train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(cross_entropy)\n",
    "#train_step = tf.train.AdagradOptimizer(1e-5).minimize(cross_entropy)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# 启动session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):#20000\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x:batch[0], y: batch[1], keep_prob: 1.0})\n",
    "            print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    print (\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
